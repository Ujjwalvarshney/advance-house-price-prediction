{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_train= pd.read_csv(r\"C:\\Users\\ujjawalv\\Documents\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_test=pd.read_csv(r\"C:\\Users\\ujjawalv\\Documents\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_test[\"SalePrice\"]=np.NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_test[\"data\"]=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_train[\"data\"]=\"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld=pd.concat([ld_train,ld_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Id', 2919, dtype('int64')),\n",
       " ('MSSubClass', 16, dtype('int64')),\n",
       " ('MSZoning', 5, dtype('O')),\n",
       " ('LotFrontage', 128, dtype('float64')),\n",
       " ('LotArea', 1951, dtype('int64')),\n",
       " ('Street', 2, dtype('O')),\n",
       " ('Alley', 2, dtype('O')),\n",
       " ('LotShape', 4, dtype('O')),\n",
       " ('LandContour', 4, dtype('O')),\n",
       " ('Utilities', 2, dtype('O')),\n",
       " ('LotConfig', 5, dtype('O')),\n",
       " ('LandSlope', 3, dtype('O')),\n",
       " ('Neighborhood', 25, dtype('O')),\n",
       " ('Condition1', 9, dtype('O')),\n",
       " ('Condition2', 8, dtype('O')),\n",
       " ('BldgType', 5, dtype('O')),\n",
       " ('HouseStyle', 8, dtype('O')),\n",
       " ('OverallQual', 10, dtype('int64')),\n",
       " ('OverallCond', 9, dtype('int64')),\n",
       " ('YearBuilt', 118, dtype('int64')),\n",
       " ('YearRemodAdd', 61, dtype('int64')),\n",
       " ('RoofStyle', 6, dtype('O')),\n",
       " ('RoofMatl', 8, dtype('O')),\n",
       " ('Exterior1st', 15, dtype('O')),\n",
       " ('Exterior2nd', 16, dtype('O')),\n",
       " ('MasVnrType', 4, dtype('O')),\n",
       " ('MasVnrArea', 444, dtype('float64')),\n",
       " ('ExterQual', 4, dtype('O')),\n",
       " ('ExterCond', 5, dtype('O')),\n",
       " ('Foundation', 6, dtype('O')),\n",
       " ('BsmtQual', 4, dtype('O')),\n",
       " ('BsmtCond', 4, dtype('O')),\n",
       " ('BsmtExposure', 4, dtype('O')),\n",
       " ('BsmtFinType1', 6, dtype('O')),\n",
       " ('BsmtFinSF1', 991, dtype('float64')),\n",
       " ('BsmtFinType2', 6, dtype('O')),\n",
       " ('BsmtFinSF2', 272, dtype('float64')),\n",
       " ('BsmtUnfSF', 1135, dtype('float64')),\n",
       " ('TotalBsmtSF', 1058, dtype('float64')),\n",
       " ('Heating', 6, dtype('O')),\n",
       " ('HeatingQC', 5, dtype('O')),\n",
       " ('CentralAir', 2, dtype('O')),\n",
       " ('Electrical', 5, dtype('O')),\n",
       " ('1stFlrSF', 1083, dtype('int64')),\n",
       " ('2ndFlrSF', 635, dtype('int64')),\n",
       " ('LowQualFinSF', 36, dtype('int64')),\n",
       " ('GrLivArea', 1292, dtype('int64')),\n",
       " ('BsmtFullBath', 4, dtype('float64')),\n",
       " ('BsmtHalfBath', 3, dtype('float64')),\n",
       " ('FullBath', 5, dtype('int64')),\n",
       " ('HalfBath', 3, dtype('int64')),\n",
       " ('BedroomAbvGr', 8, dtype('int64')),\n",
       " ('KitchenAbvGr', 4, dtype('int64')),\n",
       " ('KitchenQual', 4, dtype('O')),\n",
       " ('TotRmsAbvGrd', 14, dtype('int64')),\n",
       " ('Functional', 7, dtype('O')),\n",
       " ('Fireplaces', 5, dtype('int64')),\n",
       " ('FireplaceQu', 5, dtype('O')),\n",
       " ('GarageType', 6, dtype('O')),\n",
       " ('GarageYrBlt', 103, dtype('float64')),\n",
       " ('GarageFinish', 3, dtype('O')),\n",
       " ('GarageCars', 6, dtype('float64')),\n",
       " ('GarageArea', 603, dtype('float64')),\n",
       " ('GarageQual', 5, dtype('O')),\n",
       " ('GarageCond', 5, dtype('O')),\n",
       " ('PavedDrive', 3, dtype('O')),\n",
       " ('WoodDeckSF', 379, dtype('int64')),\n",
       " ('OpenPorchSF', 252, dtype('int64')),\n",
       " ('EnclosedPorch', 183, dtype('int64')),\n",
       " ('3SsnPorch', 31, dtype('int64')),\n",
       " ('ScreenPorch', 121, dtype('int64')),\n",
       " ('PoolArea', 14, dtype('int64')),\n",
       " ('PoolQC', 3, dtype('O')),\n",
       " ('Fence', 4, dtype('O')),\n",
       " ('MiscFeature', 4, dtype('O')),\n",
       " ('MiscVal', 38, dtype('int64')),\n",
       " ('MoSold', 12, dtype('int64')),\n",
       " ('YrSold', 5, dtype('int64')),\n",
       " ('SaleType', 9, dtype('O')),\n",
       " ('SaleCondition', 6, dtype('O')),\n",
       " ('SalePrice', 663, dtype('float64')),\n",
       " ('data', 2, dtype('O'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(ld.columns,ld.nunique(),ld.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning: 0.0014% missing values\n",
      "Alley: 0.9322% missing values\n",
      "Utilities: 0.0007% missing values\n",
      "MasVnrType: 0.0082% missing values\n",
      "BsmtQual: 0.0277% missing values\n",
      "BsmtCond: 0.0281% missing values\n",
      "BsmtExposure: 0.0281% missing values\n",
      "BsmtFinType1: 0.0271% missing values\n",
      "BsmtFinType2: 0.0274% missing values\n",
      "Functional: 0.0007% missing values\n",
      "FireplaceQu: 0.4865% missing values\n",
      "GarageType: 0.0538% missing values\n",
      "GarageFinish: 0.0545% missing values\n",
      "GarageQual: 0.0545% missing values\n",
      "GarageCond: 0.0545% missing values\n",
      "PoolQC: 0.9966% missing values\n",
      "Fence: 0.8044% missing values\n",
      "MiscFeature: 0.964% missing values\n"
     ]
    }
   ],
   "source": [
    "## Let us capture all the nan values\n",
    "## First lets handle Categorical features which are missing\n",
    "features_nan=[feature for feature in ld.columns if ld[feature].isnull().sum()>1 and ld[feature].dtypes=='O']\n",
    "\n",
    "for feature in features_nan:\n",
    "    print(\"{}: {}% missing values\".format(feature,np.round(ld[feature].isnull().mean(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can seee the data which high percentage of missing values we drop that\n",
    "ld.drop([\"Alley\",\"PoolQC\",\"Fence\",\"MiscFeature\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n",
       "       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n",
       "       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
       "       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n",
       "       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
       "       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
       "       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition', 'data'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld.select_dtypes([\"object\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LotFrontage: 0.1665% missing value\n",
      "MasVnrArea: 0.0079% missing value\n",
      "BsmtFullBath: 0.0007% missing value\n",
      "BsmtHalfBath: 0.0007% missing value\n",
      "GarageYrBlt: 0.0545% missing value\n",
      "SalePrice: 0.4998% missing value\n"
     ]
    }
   ],
   "source": [
    "## Now lets check for numerical variables the contains missing values\n",
    "numerical_with_nan=[feature for feature in ld.columns if ld[feature].isnull().sum()>1 and ld[feature].dtypes!='O']\n",
    "\n",
    "## We will print the numerical nan variables and percentage of missing values\n",
    "\n",
    "for feature in numerical_with_nan:\n",
    "    print(\"{}: {}% missing value\".format(feature,np.around(ld[feature].isnull().mean(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temporal Variables (Date Time Variables)\n",
    "\n",
    "for feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n",
    "       \n",
    "    ld[feature]=ld['YrSold']-ld[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YearBuilt  YearRemodAdd  GarageYrBlt\n",
       "0          5             5          5.0\n",
       "1         31            31         31.0\n",
       "2          7             6          7.0\n",
       "3         91            36          8.0\n",
       "4          8             8          8.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.0     276\n",
       "80.0     137\n",
       "70.0     133\n",
       "50.0     117\n",
       "75.0     105\n",
       "        ... \n",
       "137.0      1\n",
       "182.0      1\n",
       "119.0      1\n",
       "195.0      1\n",
       "141.0      1\n",
       "Name: LotFrontage, Length: 128, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld[\"LotFrontage\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0   1          60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "1   2          20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "2   3          60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "3   4          70       RL         60.0     9550   Pave      IR1         Lvl   \n",
       "4   5          60       RL         84.0    14260   Pave      IR1         Lvl   \n",
       "\n",
       "  Utilities LotConfig  ... 3SsnPorch ScreenPorch PoolArea MiscVal MoSold  \\\n",
       "0    AllPub    Inside  ...         0           0        0       0      2   \n",
       "1    AllPub       FR2  ...         0           0        0       0      5   \n",
       "2    AllPub    Inside  ...         0           0        0       0      9   \n",
       "3    AllPub    Corner  ...         0           0        0       0      2   \n",
       "4    AllPub       FR2  ...         0           0        0       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice   data  \n",
       "0   2008        WD         Normal   208500.0  train  \n",
       "1   2007        WD         Normal   181500.0  train  \n",
       "2   2008        WD         Normal   223500.0  train  \n",
       "3   2006        WD        Abnorml   140000.0  train  \n",
       "4   2008        WD         Normal   250000.0  train  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\n",
    "\n",
    "for feature in num_features:\n",
    "    ld[feature]=np.log(ld[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=ld.select_dtypes([\"object\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n",
       "       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n",
       "       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
       "       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n",
       "       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
       "       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
       "       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition', 'data'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=cat_cols[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition1\n",
      "Condition2\n",
      "BldgType\n",
      "HouseStyle\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "ExterQual\n",
      "ExterCond\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n",
      "SaleType\n",
      "SaleCondition\n"
     ]
    }
   ],
   "source": [
    "for col in cat_cols:\n",
    "    freqs=ld[col].value_counts()\n",
    "    k=freqs.index[freqs>100]\n",
    "    for cat in k:\n",
    "        name=col+'_'+cat\n",
    "        ld[name]=(ld[col]==cat).astype(int)\n",
    "    del ld[col]\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotFrontage      486\n",
       "MasVnrArea        23\n",
       "BsmtFinSF1         1\n",
       "BsmtFinSF2         1\n",
       "BsmtUnfSF          1\n",
       "TotalBsmtSF        1\n",
       "BsmtFullBath       2\n",
       "BsmtHalfBath       2\n",
       "GarageYrBlt      159\n",
       "GarageCars         1\n",
       "GarageArea         1\n",
       "SalePrice       1459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_columns=ld.columns[ld.isnull().any()]\n",
    "ld[null_columns].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x268669ad3c8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(ld.isnull(),yticklabels=False,cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld[\"LotFrontage\"]=ld[\"LotFrontage\"].fillna(ld[\"LotFrontage\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0      192\n",
       " 0.0      150\n",
       " 2.0       82\n",
       " 4.0       81\n",
       " 5.0       72\n",
       "         ... \n",
       " 107.0      1\n",
       "-200.0      1\n",
       "-1.0        1\n",
       " 108.0      1\n",
       " 109.0      1\n",
       "Name: GarageYrBlt, Length: 111, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld[\"GarageYrBlt\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld[\"GarageYrBlt\"]=ld[\"GarageYrBlt\"].fillna(ld[\"GarageYrBlt\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0      1738\n",
       "120.0      15\n",
       "176.0      13\n",
       "200.0      13\n",
       "216.0      12\n",
       "         ... \n",
       "664.0       1\n",
       "247.0       1\n",
       "550.0       1\n",
       "137.0       1\n",
       "572.0       1\n",
       "Name: MasVnrArea, Length: 444, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld[\"MasVnrArea\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld[\"MasVnrArea\"]=ld[\"MasVnrArea\"].fillna(ld[\"MasVnrArea\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ld.columns:\n",
    "    if (col not in [\"SalePrice\",'data'])& (ld[col].isnull().sum()>0):\n",
    "        ld.loc[ld[col].isnull(),col]=ld.loc[ld['data']=='train',col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_scale=[feature for feature in ld.columns if feature not in ['Id','SalePrice','data']]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(ld[feature_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23529412, 0.41820812, 0.3663439 , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.49506375, 0.39131677, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.23529412, 0.434909  , 0.4223585 , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.75162522, 0.53496717, ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.38235294, 0.40071794, 0.40775256, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.23529412, 0.46620707, 0.39186645, ..., 1.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(ld[feature_scale])# transform the train and test set, and add on the Id and SalePrice variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the train and test set, and add on the Id and SalePrice variables\n",
    "ld = pd.concat([ld[['Id', 'SalePrice','data']].reset_index(drop=True),\n",
    "                    pd.DataFrame(scaler.transform(ld[feature_scale]), columns=feature_scale)],\n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>data</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageQual_TA</th>\n",
       "      <th>GarageQual_Fa</th>\n",
       "      <th>GarageCond_TA</th>\n",
       "      <th>PavedDrive_Y</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.247694</td>\n",
       "      <td>train</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.418208</td>\n",
       "      <td>0.366344</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12.109011</td>\n",
       "      <td>train</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495064</td>\n",
       "      <td>0.391317</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.233577</td>\n",
       "      <td>0.532258</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12.317167</td>\n",
       "      <td>train</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.434909</td>\n",
       "      <td>0.422359</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.058394</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>11.849398</td>\n",
       "      <td>train</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.388581</td>\n",
       "      <td>0.390295</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.671533</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12.429216</td>\n",
       "      <td>train</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.513123</td>\n",
       "      <td>0.468761</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SalePrice   data  MSSubClass  LotFrontage   LotArea  OverallQual  \\\n",
       "0   1  12.247694  train    0.235294     0.418208  0.366344     0.666667   \n",
       "1   2  12.109011  train    0.000000     0.495064  0.391317     0.555556   \n",
       "2   3  12.317167  train    0.235294     0.434909  0.422359     0.666667   \n",
       "3   4  11.849398  train    0.294118     0.388581  0.390295     0.666667   \n",
       "4   5  12.429216  train    0.235294     0.513123  0.468761     0.777778   \n",
       "\n",
       "   OverallCond  YearBuilt  YearRemodAdd  ...  GarageQual_TA  GarageQual_Fa  \\\n",
       "0        0.500   0.043796      0.112903  ...            1.0            0.0   \n",
       "1        0.875   0.233577      0.532258  ...            1.0            0.0   \n",
       "2        0.500   0.058394      0.129032  ...            1.0            0.0   \n",
       "3        0.500   0.671533      0.612903  ...            1.0            0.0   \n",
       "4        0.500   0.065693      0.161290  ...            1.0            0.0   \n",
       "\n",
       "   GarageCond_TA  PavedDrive_Y  PavedDrive_N  SaleType_WD  SaleType_New  \\\n",
       "0            1.0           1.0           0.0          1.0           0.0   \n",
       "1            1.0           1.0           0.0          1.0           0.0   \n",
       "2            1.0           1.0           0.0          1.0           0.0   \n",
       "3            1.0           1.0           0.0          1.0           0.0   \n",
       "4            1.0           1.0           0.0          1.0           0.0   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  SaleCondition_Abnorml  \n",
       "0                   1.0                    0.0                    0.0  \n",
       "1                   1.0                    0.0                    0.0  \n",
       "2                   1.0                    0.0                    0.0  \n",
       "3                   0.0                    0.0                    1.0  \n",
       "4                   1.0                    0.0                    0.0  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_train=ld[ld[\"data\"]==\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_test=ld[ld[\"data\"]==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ujjawalv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "ld_test.drop([\"SalePrice\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_train.drop(['data'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_test.drop(['data'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(ld_train,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train.drop([\"SalePrice\",\"Id\"],axis=1)\n",
    "y_train=train[\"SalePrice\"]\n",
    "x_test=test.drop([\"SalePrice\",\"Id\"],axis=1)\n",
    "y_test=test[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm= LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr=lm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016705180865860682"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " mean_squared_error(logr,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for feature slection\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# to visualise al the columns in the dataframe\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168, 148)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=Lasso(alpha=0.005, copy_X=True, fit_intercept=True,\n",
       "                                max_iter=1000, normalize=False, positive=False,\n",
       "                                precompute=False, random_state=0,\n",
       "                                selection='cyclic', tol=0.0001,\n",
       "                                warm_start=False),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, I specify the Lasso Regression model, and I\n",
    "# select a suitable alpha (equivalent of penalty).\n",
    "# The bigger the alpha the less features that will be selected.\n",
    "\n",
    "# Then I use the selectFromModel object from sklearn, which\n",
    "# will select the features which coefficients are non-zero\n",
    "\n",
    "feature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # remember to set the seed, the random state in this function\n",
    "feature_sel_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False,  True, False,  True,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True,  True, False, False,  True, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False,  True, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False,  True, False, False, False, False,  True, False,\n",
       "       False,  True, False,  True, False, False, False,  True, False,\n",
       "        True, False,  True,  True, False, False, False, False, False,\n",
       "       False, False,  True, False, False,  True, False, False, False,\n",
       "        True, False,  True,  True,  True,  True,  True, False, False,\n",
       "        True, False,  True, False, False,  True,  True,  True, False,\n",
       "       False, False, False,  True])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sel_model.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 148\n",
      "selected features: 37\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sel_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-c3ac2f688b90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'selected features: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m print('features with coefficients shrank to zero: {}'.format(\n\u001b[1;32m---> 10\u001b[1;33m     np.sum(sel_.estimator_.coef_ == 0)))\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sel_' is not defined"
     ]
    }
   ],
   "source": [
    "# let's print the number of total and selected features\n",
    "\n",
    "# this is how we can make a list of the selected features\n",
    "selected_feat = x_train.columns[(feature_sel_model.get_support())]\n",
    "\n",
    "# let's print some stats\n",
    "print('total features: {}'.format((x_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OverallQual', 'YearRemodAdd', 'GrLivArea', 'Fireplaces', 'GarageCars',\n",
       "       'GarageArea', 'MSZoning_RL', 'MSZoning_RM', 'LotShape_Reg',\n",
       "       'LotConfig_Inside', 'Neighborhood_Edwards', 'Condition1_Norm',\n",
       "       'BldgType_1Fam', 'RoofStyle_Gable', 'MasVnrType_None', 'ExterQual_TA',\n",
       "       'Foundation_PConc', 'BsmtQual_TA', 'BsmtQual_Ex', 'BsmtExposure_No',\n",
       "       'BsmtExposure_Gd', 'BsmtFinType1_Unf', 'BsmtFinType1_GLQ',\n",
       "       'HeatingQC_Ex', 'CentralAir_Y', 'KitchenQual_TA', 'KitchenQual_Ex',\n",
       "       'Functional_Typ', 'FireplaceQu_Gd', 'FireplaceQu_TA',\n",
       "       'GarageType_Attchd', 'GarageFinish_Unf', 'GarageFinish_Fin',\n",
       "       'GarageCond_TA', 'PavedDrive_Y', 'PavedDrive_N',\n",
       "       'SaleCondition_Abnorml'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x_test[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>MSZoning_RL</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>LotConfig_Inside</th>\n",
       "      <th>Neighborhood_Edwards</th>\n",
       "      <th>Condition1_Norm</th>\n",
       "      <th>BldgType_1Fam</th>\n",
       "      <th>RoofStyle_Gable</th>\n",
       "      <th>MasVnrType_None</th>\n",
       "      <th>ExterQual_TA</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>BsmtQual_TA</th>\n",
       "      <th>BsmtQual_Ex</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>BsmtExposure_Gd</th>\n",
       "      <th>BsmtFinType1_Unf</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>HeatingQC_Ex</th>\n",
       "      <th>CentralAir_Y</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>KitchenQual_Ex</th>\n",
       "      <th>Functional_Typ</th>\n",
       "      <th>FireplaceQu_Gd</th>\n",
       "      <th>FireplaceQu_TA</th>\n",
       "      <th>GarageType_Attchd</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>GarageFinish_Fin</th>\n",
       "      <th>GarageCond_TA</th>\n",
       "      <th>PavedDrive_Y</th>\n",
       "      <th>PavedDrive_N</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.464593</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.436753</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.486675</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.295699</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1413</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.604401</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1389</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.457691</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.295699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      OverallQual  YearRemodAdd  GrLivArea  Fireplaces  GarageCars  \\\n",
       "455      0.666667      0.580645   0.464593        0.25         0.4   \n",
       "863      0.444444      0.838710   0.436753        0.00         0.2   \n",
       "504      0.555556      0.596774   0.486675        0.25         0.4   \n",
       "1413     0.777778      0.080645   0.604401        0.25         0.4   \n",
       "1389     0.555556      0.951613   0.457691        0.25         0.4   \n",
       "\n",
       "      GarageArea  MSZoning_RL  MSZoning_RM  LotShape_Reg  LotConfig_Inside  \\\n",
       "455     0.354839          1.0          0.0           1.0               1.0   \n",
       "863     0.451613          1.0          0.0           1.0               1.0   \n",
       "504     0.295699          1.0          0.0           1.0               1.0   \n",
       "1413    0.416667          1.0          0.0           0.0               0.0   \n",
       "1389    0.295699          0.0          1.0           1.0               1.0   \n",
       "\n",
       "      Neighborhood_Edwards  Condition1_Norm  BldgType_1Fam  RoofStyle_Gable  \\\n",
       "455                    0.0              1.0            1.0              0.0   \n",
       "863                    0.0              1.0            1.0              0.0   \n",
       "504                    0.0              1.0            0.0              1.0   \n",
       "1413                   0.0              1.0            1.0              1.0   \n",
       "1389                   0.0              1.0            1.0              1.0   \n",
       "\n",
       "      MasVnrType_None  ExterQual_TA  Foundation_PConc  BsmtQual_TA  \\\n",
       "455               0.0           1.0               0.0          1.0   \n",
       "863               1.0           1.0               0.0          1.0   \n",
       "504               1.0           1.0               0.0          1.0   \n",
       "1413              0.0           0.0               1.0          0.0   \n",
       "1389              1.0           1.0               0.0          1.0   \n",
       "\n",
       "      BsmtQual_Ex  BsmtExposure_No  BsmtExposure_Gd  BsmtFinType1_Unf  \\\n",
       "455           0.0              1.0              0.0               0.0   \n",
       "863           0.0              1.0              0.0               0.0   \n",
       "504           0.0              1.0              0.0               0.0   \n",
       "1413          0.0              1.0              0.0               0.0   \n",
       "1389          0.0              1.0              0.0               0.0   \n",
       "\n",
       "      BsmtFinType1_GLQ  HeatingQC_Ex  CentralAir_Y  KitchenQual_TA  \\\n",
       "455                0.0           0.0           1.0             1.0   \n",
       "863                0.0           0.0           1.0             1.0   \n",
       "504                0.0           0.0           1.0             1.0   \n",
       "1413               1.0           1.0           1.0             0.0   \n",
       "1389               0.0           1.0           1.0             1.0   \n",
       "\n",
       "      KitchenQual_Ex  Functional_Typ  FireplaceQu_Gd  FireplaceQu_TA  \\\n",
       "455              0.0             1.0             0.0             1.0   \n",
       "863              0.0             1.0             0.0             0.0   \n",
       "504              0.0             1.0             0.0             0.0   \n",
       "1413             0.0             1.0             1.0             0.0   \n",
       "1389             0.0             1.0             1.0             0.0   \n",
       "\n",
       "      GarageType_Attchd  GarageFinish_Unf  GarageFinish_Fin  GarageCond_TA  \\\n",
       "455                 1.0               1.0               0.0            1.0   \n",
       "863                 1.0               1.0               0.0            1.0   \n",
       "504                 1.0               1.0               0.0            1.0   \n",
       "1413                1.0               0.0               1.0            1.0   \n",
       "1389                0.0               1.0               0.0            1.0   \n",
       "\n",
       "      PavedDrive_Y  PavedDrive_N  SaleCondition_Abnorml  \n",
       "455            1.0           0.0                    0.0  \n",
       "863            1.0           0.0                    0.0  \n",
       "504            1.0           0.0                    0.0  \n",
       "1413           1.0           0.0                    1.0  \n",
       "1389           1.0           0.0                    0.0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr=lm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019057440703722277"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(logr,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "regressor=xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster=['gbtree','gblinear']\n",
    "base_score=[0.25,0.5,0.75,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper Parameter Optimization\n",
    "\n",
    "\n",
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "booster=['gbtree','gblinear']\n",
    "learning_rate=[0.05,0.1,0.15,0.20]\n",
    "min_child_weight=[1,2,3,4]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the random search with 4-fold cross validation\n",
    "random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "            param_distributions=hyperparameter_grid,\n",
    "            cv=5, n_iter=50,\n",
    "            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   53.1s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None, gamma=None,\n",
       "                                          gpu_id=None, importance_type='gain',\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_con...\n",
       "                   iid='warn', n_iter=50, n_jobs=4,\n",
       "                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
       "                                        'max_depth': [2, 3, 5, 10, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4],\n",
       "                                        'n_estimators': [100, 500, 900, 1100,\n",
       "                                                         1500]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.05, max_delta_step=0, max_depth=2,\n",
       "             min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=900, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "             validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.05, max_delta_step=0, max_depth=2,\n",
       "             min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=900, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "             validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
    "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=900, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
       "             objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "             scale_pos_weight=1, seed=0, silent=True, subsample=1,\n",
       "             tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr=regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020943752334569316"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(logr,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ujjawalv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=37, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\ujjawalv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\ujjawalv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, kernel_initializer=\"he_uniform\")`\n",
      "C:\\Users\\ujjawalv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=1, kernel_initializer=\"he_uniform\")`\n",
      "C:\\Users\\ujjawalv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 3.6525 - val_loss: 0.9217\n",
      "Epoch 2/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.8246 - val_loss: 0.7151\n",
      "Epoch 3/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.6172 - val_loss: 0.6419\n",
      "Epoch 4/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.5590 - val_loss: 0.6478\n",
      "Epoch 5/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.4772 - val_loss: 0.5037\n",
      "Epoch 6/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.4307 - val_loss: 0.4902\n",
      "Epoch 7/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.3980 - val_loss: 0.4201\n",
      "Epoch 8/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.3678 - val_loss: 0.4167\n",
      "Epoch 9/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.3335 - val_loss: 0.3765\n",
      "Epoch 10/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.3127 - val_loss: 0.3640\n",
      "Epoch 11/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.3160 - val_loss: 0.3542\n",
      "Epoch 12/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.2821 - val_loss: 0.3420\n",
      "Epoch 13/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.2838 - val_loss: 0.3059\n",
      "Epoch 14/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.2580 - val_loss: 0.3661\n",
      "Epoch 15/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.2425 - val_loss: 0.2927\n",
      "Epoch 16/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.2471 - val_loss: 0.3522\n",
      "Epoch 17/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.2420 - val_loss: 0.2802\n",
      "Epoch 18/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.2280 - val_loss: 0.2767\n",
      "Epoch 19/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.2225 - val_loss: 0.2942\n",
      "Epoch 20/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.2173 - val_loss: 0.2562\n",
      "Epoch 21/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.2285 - val_loss: 0.2589\n",
      "Epoch 22/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.2009 - val_loss: 0.2779\n",
      "Epoch 23/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.2098 - val_loss: 0.2888\n",
      "Epoch 24/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1961 - val_loss: 0.2456\n",
      "Epoch 25/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1904 - val_loss: 0.2421\n",
      "Epoch 26/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1978 - val_loss: 0.2568\n",
      "Epoch 27/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1967 - val_loss: 0.2801\n",
      "Epoch 28/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.1779 - val_loss: 0.2332\n",
      "Epoch 29/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1745 - val_loss: 0.2296\n",
      "Epoch 30/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1751 - val_loss: 0.2316\n",
      "Epoch 31/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1667 - val_loss: 0.2406\n",
      "Epoch 32/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1813 - val_loss: 0.2554\n",
      "Epoch 33/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1892 - val_loss: 0.2513\n",
      "Epoch 34/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1712 - val_loss: 0.2466\n",
      "Epoch 35/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1586 - val_loss: 0.2257\n",
      "Epoch 36/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.1563 - val_loss: 0.2334\n",
      "Epoch 37/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.1638 - val_loss: 0.2275\n",
      "Epoch 38/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.1739 - val_loss: 0.2697\n",
      "Epoch 39/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1519 - val_loss: 0.2283\n",
      "Epoch 40/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.1471 - val_loss: 0.2212\n",
      "Epoch 41/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1428 - val_loss: 0.2180\n",
      "Epoch 42/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.1582 - val_loss: 0.2567\n",
      "Epoch 43/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1555 - val_loss: 0.2195\n",
      "Epoch 44/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1529 - val_loss: 0.2560\n",
      "Epoch 45/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.1502 - val_loss: 0.2599\n",
      "Epoch 46/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1420 - val_loss: 0.2305\n",
      "Epoch 47/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1538 - val_loss: 0.2192\n",
      "Epoch 48/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1371 - val_loss: 0.2587\n",
      "Epoch 49/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.1415 - val_loss: 0.2189\n",
      "Epoch 50/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1340 - val_loss: 0.2412\n",
      "Epoch 51/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1363 - val_loss: 0.2141\n",
      "Epoch 52/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1387 - val_loss: 0.2098\n",
      "Epoch 53/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1476 - val_loss: 0.2558\n",
      "Epoch 54/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1437 - val_loss: 0.1989\n",
      "Epoch 55/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1230 - val_loss: 0.2069\n",
      "Epoch 56/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1207 - val_loss: 0.2069\n",
      "Epoch 57/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1249 - val_loss: 0.2021\n",
      "Epoch 58/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1387 - val_loss: 0.2175\n",
      "Epoch 59/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1245 - val_loss: 0.2048\n",
      "Epoch 60/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1245 - val_loss: 0.2175\n",
      "Epoch 61/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1215 - val_loss: 0.2060\n",
      "Epoch 62/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1315 - val_loss: 0.2061\n",
      "Epoch 63/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1213 - val_loss: 0.2051\n",
      "Epoch 64/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1227 - val_loss: 0.2078\n",
      "Epoch 65/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.1143 - val_loss: 0.1988\n",
      "Epoch 66/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1183 - val_loss: 0.2327\n",
      "Epoch 67/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1188 - val_loss: 0.2027\n",
      "Epoch 68/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.1112 - val_loss: 0.1963\n",
      "Epoch 69/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1127 - val_loss: 0.2099\n",
      "Epoch 70/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1195 - val_loss: 0.2199\n",
      "Epoch 71/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1109 - val_loss: 0.2598\n",
      "Epoch 72/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.1162 - val_loss: 0.1993\n",
      "Epoch 73/1000\n",
      "934/934 [==============================] - 35s 38ms/step - loss: 0.1004 - val_loss: 0.2032\n",
      "Epoch 74/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.1211 - val_loss: 0.2052\n",
      "Epoch 75/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.1102 - val_loss: 0.2241\n",
      "Epoch 76/1000\n",
      "934/934 [==============================] - 35s 38ms/step - loss: 0.1166 - val_loss: 0.1940\n",
      "Epoch 77/1000\n",
      "934/934 [==============================] - 34s 37ms/step - loss: 0.1107 - val_loss: 0.2031\n",
      "Epoch 78/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 33s 36ms/step - loss: 0.1047 - val_loss: 0.2149\n",
      "Epoch 79/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.1064 - val_loss: 0.1980\n",
      "Epoch 80/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.1060 - val_loss: 0.2031\n",
      "Epoch 81/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.1215 - val_loss: 0.2075\n",
      "Epoch 82/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1212 - val_loss: 0.1943\n",
      "Epoch 83/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.1010 - val_loss: 0.1967\n",
      "Epoch 84/1000\n",
      "934/934 [==============================] - 36s 38ms/step - loss: 0.1035 - val_loss: 0.2078\n",
      "Epoch 85/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.1023 - val_loss: 0.1925\n",
      "Epoch 86/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1182 - val_loss: 0.2399\n",
      "Epoch 87/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.1082 - val_loss: 0.1905\n",
      "Epoch 88/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1022 - val_loss: 0.2068\n",
      "Epoch 89/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.1030 - val_loss: 0.1979\n",
      "Epoch 90/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0994 - val_loss: 0.1943\n",
      "Epoch 91/1000\n",
      "934/934 [==============================] - 34s 37ms/step - loss: 0.1259 - val_loss: 0.1952\n",
      "Epoch 92/1000\n",
      "934/934 [==============================] - 36s 39ms/step - loss: 0.0962 - val_loss: 0.1948\n",
      "Epoch 93/1000\n",
      "934/934 [==============================] - 36s 38ms/step - loss: 0.1084 - val_loss: 0.2002\n",
      "Epoch 94/1000\n",
      "934/934 [==============================] - 36s 38ms/step - loss: 0.1004 - val_loss: 0.1946\n",
      "Epoch 95/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.1047 - val_loss: 0.1972\n",
      "Epoch 96/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.1084 - val_loss: 0.1859\n",
      "Epoch 97/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1098 - val_loss: 0.2526\n",
      "Epoch 98/1000\n",
      "934/934 [==============================] - 36s 39ms/step - loss: 0.1048 - val_loss: 0.1986\n",
      "Epoch 99/1000\n",
      "934/934 [==============================] - 32s 35ms/step - loss: 0.0952 - val_loss: 0.1908\n",
      "Epoch 100/1000\n",
      "934/934 [==============================] - 36s 38ms/step - loss: 0.0966 - val_loss: 0.1919\n",
      "Epoch 101/1000\n",
      "934/934 [==============================] - 35s 38ms/step - loss: 0.0929 - val_loss: 0.1917\n",
      "Epoch 102/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.1007 - val_loss: 0.1923\n",
      "Epoch 103/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.1070 - val_loss: 0.2142\n",
      "Epoch 104/1000\n",
      "934/934 [==============================] - 36s 39ms/step - loss: 0.1103 - val_loss: 0.1921\n",
      "Epoch 105/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.1012 - val_loss: 0.1914\n",
      "Epoch 106/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0898 - val_loss: 0.1920\n",
      "Epoch 107/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0953 - val_loss: 0.1888\n",
      "Epoch 108/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1037 - val_loss: 0.1853\n",
      "Epoch 109/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0958 - val_loss: 0.2175\n",
      "Epoch 110/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1016 - val_loss: 0.2002\n",
      "Epoch 111/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0895 - val_loss: 0.1955\n",
      "Epoch 112/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0982 - val_loss: 0.1840\n",
      "Epoch 113/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0867 - val_loss: 0.1923\n",
      "Epoch 114/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0823 - val_loss: 0.1816\n",
      "Epoch 115/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0901 - val_loss: 0.1829\n",
      "Epoch 116/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1094 - val_loss: 0.1872\n",
      "Epoch 117/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0895 - val_loss: 0.1950\n",
      "Epoch 118/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0943 - val_loss: 0.1856\n",
      "Epoch 119/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0925 - val_loss: 0.1851\n",
      "Epoch 120/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0861 - val_loss: 0.1801\n",
      "Epoch 121/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0978 - val_loss: 0.1833\n",
      "Epoch 122/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0905 - val_loss: 0.1998\n",
      "Epoch 123/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0921 - val_loss: 0.1905\n",
      "Epoch 124/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1180 - val_loss: 0.1942\n",
      "Epoch 125/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0928 - val_loss: 0.2005\n",
      "Epoch 126/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0897 - val_loss: 0.1830\n",
      "Epoch 127/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0883 - val_loss: 0.1959\n",
      "Epoch 128/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0937 - val_loss: 0.2031\n",
      "Epoch 129/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0995 - val_loss: 0.1865\n",
      "Epoch 130/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0904 - val_loss: 0.1878\n",
      "Epoch 131/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0917 - val_loss: 0.1831\n",
      "Epoch 132/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0901 - val_loss: 0.1814\n",
      "Epoch 133/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0948 - val_loss: 0.1910\n",
      "Epoch 134/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0966 - val_loss: 0.1891\n",
      "Epoch 135/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0803 - val_loss: 0.1826\n",
      "Epoch 136/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0852 - val_loss: 0.1842\n",
      "Epoch 137/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0880 - val_loss: 0.1815\n",
      "Epoch 138/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0864 - val_loss: 0.1831\n",
      "Epoch 139/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0873 - val_loss: 0.1751\n",
      "Epoch 140/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0813 - val_loss: 0.1841\n",
      "Epoch 141/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0849 - val_loss: 0.1813\n",
      "Epoch 142/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0946 - val_loss: 0.1882\n",
      "Epoch 143/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0797 - val_loss: 0.1781\n",
      "Epoch 144/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0915 - val_loss: 0.2076\n",
      "Epoch 145/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0840 - val_loss: 0.1772\n",
      "Epoch 146/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0772 - val_loss: 0.2186\n",
      "Epoch 147/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0838 - val_loss: 0.1826\n",
      "Epoch 148/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0842 - val_loss: 0.1913\n",
      "Epoch 149/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.1005 - val_loss: 0.1775\n",
      "Epoch 150/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0853 - val_loss: 0.1810\n",
      "Epoch 151/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0805 - val_loss: 0.1776\n",
      "Epoch 152/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0831 - val_loss: 0.1971\n",
      "Epoch 153/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0782 - val_loss: 0.1933\n",
      "Epoch 154/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0839 - val_loss: 0.1896\n",
      "Epoch 155/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0862 - val_loss: 0.1776\n",
      "Epoch 156/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0749 - val_loss: 0.1978\n",
      "Epoch 157/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0936 - val_loss: 0.1799\n",
      "Epoch 158/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0884 - val_loss: 0.1749\n",
      "Epoch 159/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0780 - val_loss: 0.1803\n",
      "Epoch 160/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0883 - val_loss: 0.1884\n",
      "Epoch 161/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0807 - val_loss: 0.1732\n",
      "Epoch 162/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0830 - val_loss: 0.1766\n",
      "Epoch 163/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0718 - val_loss: 0.1837\n",
      "Epoch 164/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0745 - val_loss: 0.1779\n",
      "Epoch 165/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.0752 - val_loss: 0.1956\n",
      "Epoch 166/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0839 - val_loss: 0.2049\n",
      "Epoch 167/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0871 - val_loss: 0.1769\n",
      "Epoch 168/1000\n",
      "934/934 [==============================] - 35s 38ms/step - loss: 0.0752 - val_loss: 0.1788\n",
      "Epoch 169/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0738 - val_loss: 0.1774\n",
      "Epoch 170/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0973 - val_loss: 0.1771\n",
      "Epoch 171/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0883 - val_loss: 0.1823\n",
      "Epoch 172/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0824 - val_loss: 0.1873\n",
      "Epoch 173/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0807 - val_loss: 0.2130\n",
      "Epoch 174/1000\n",
      "934/934 [==============================] - 32s 35ms/step - loss: 0.0973 - val_loss: 0.1934\n",
      "Epoch 175/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0813 - val_loss: 0.1736\n",
      "Epoch 176/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0783 - val_loss: 0.1764\n",
      "Epoch 177/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0789 - val_loss: 0.1725\n",
      "Epoch 178/1000\n",
      "934/934 [==============================] - 39s 41ms/step - loss: 0.0765 - val_loss: 0.1723\n",
      "Epoch 179/1000\n",
      "934/934 [==============================] - 36s 39ms/step - loss: 0.0774 - val_loss: 0.1906\n",
      "Epoch 180/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0905 - val_loss: 0.1764\n",
      "Epoch 181/1000\n",
      "934/934 [==============================] - 32s 35ms/step - loss: 0.0908 - val_loss: 0.1845\n",
      "Epoch 182/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0812 - val_loss: 0.2150\n",
      "Epoch 183/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0828 - val_loss: 0.1712\n",
      "Epoch 184/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0805 - val_loss: 0.1991\n",
      "Epoch 185/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0941 - val_loss: 0.1791\n",
      "Epoch 186/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0783 - val_loss: 0.1684\n",
      "Epoch 187/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0759 - val_loss: 0.1811\n",
      "Epoch 188/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0937 - val_loss: 0.1792\n",
      "Epoch 189/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0754 - val_loss: 0.1775\n",
      "Epoch 190/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0749 - val_loss: 0.1742\n",
      "Epoch 191/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0768 - val_loss: 0.1745\n",
      "Epoch 192/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0814 - val_loss: 0.1823\n",
      "Epoch 193/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0790 - val_loss: 0.1821\n",
      "Epoch 194/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0792 - val_loss: 0.1842\n",
      "Epoch 195/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0755 - val_loss: 0.1672\n",
      "Epoch 196/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0720 - val_loss: 0.1776\n",
      "Epoch 197/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0836 - val_loss: 0.1738\n",
      "Epoch 198/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0765 - val_loss: 0.1967\n",
      "Epoch 199/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0825 - val_loss: 0.1788\n",
      "Epoch 200/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0744 - val_loss: 0.1664\n",
      "Epoch 201/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0721 - val_loss: 0.1650\n",
      "Epoch 202/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0786 - val_loss: 0.1741\n",
      "Epoch 203/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0897 - val_loss: 0.2057\n",
      "Epoch 204/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0849 - val_loss: 0.1797\n",
      "Epoch 205/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0712 - val_loss: 0.1985\n",
      "Epoch 206/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0864 - val_loss: 0.1679\n",
      "Epoch 207/1000\n",
      "934/934 [==============================] - 29s 32ms/step - loss: 0.0733 - val_loss: 0.1848\n",
      "Epoch 208/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0657 - val_loss: 0.1692\n",
      "Epoch 209/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0811 - val_loss: 0.1716\n",
      "Epoch 210/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0763 - val_loss: 0.1758\n",
      "Epoch 211/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0723 - val_loss: 0.1743\n",
      "Epoch 212/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0695 - val_loss: 0.1747\n",
      "Epoch 213/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0821 - val_loss: 0.1843\n",
      "Epoch 214/1000\n",
      "934/934 [==============================] - 29s 32ms/step - loss: 0.0795 - val_loss: 0.1811\n",
      "Epoch 215/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0706 - val_loss: 0.1894\n",
      "Epoch 216/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0747 - val_loss: 0.1797\n",
      "Epoch 217/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0710 - val_loss: 0.1755\n",
      "Epoch 218/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0781 - val_loss: 0.1731\n",
      "Epoch 219/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0674 - val_loss: 0.1743\n",
      "Epoch 220/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0738 - val_loss: 0.1679\n",
      "Epoch 221/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0702 - val_loss: 0.1759\n",
      "Epoch 222/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0783 - val_loss: 0.1764\n",
      "Epoch 223/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0940 - val_loss: 0.2023\n",
      "Epoch 224/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0830 - val_loss: 0.1731\n",
      "Epoch 225/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0735 - val_loss: 0.1696\n",
      "Epoch 226/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0939 - val_loss: 0.1692\n",
      "Epoch 227/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0679 - val_loss: 0.1677\n",
      "Epoch 228/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0748 - val_loss: 0.1968\n",
      "Epoch 229/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0700 - val_loss: 0.1677\n",
      "Epoch 230/1000\n",
      "934/934 [==============================] - 34s 37ms/step - loss: 0.0759 - val_loss: 0.1799\n",
      "Epoch 231/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0731 - val_loss: 0.1677\n",
      "Epoch 232/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0797 - val_loss: 0.1779\n",
      "Epoch 233/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0640 - val_loss: 0.1754\n",
      "Epoch 234/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0634 - val_loss: 0.1701\n",
      "Epoch 235/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0634 - val_loss: 0.1661\n",
      "Epoch 236/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0750 - val_loss: 0.1649\n",
      "Epoch 237/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0721 - val_loss: 0.1680\n",
      "Epoch 238/1000\n",
      "934/934 [==============================] - 35s 37ms/step - loss: 0.0666 - val_loss: 0.1706\n",
      "Epoch 239/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0717 - val_loss: 0.1772\n",
      "Epoch 240/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0721 - val_loss: 0.1683\n",
      "Epoch 241/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0748 - val_loss: 0.1741\n",
      "Epoch 242/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.0739 - val_loss: 0.1748\n",
      "Epoch 243/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0725 - val_loss: 0.1779\n",
      "Epoch 244/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0576 - val_loss: 0.1662\n",
      "Epoch 245/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0889 - val_loss: 0.1732\n",
      "Epoch 246/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0662 - val_loss: 0.1699\n",
      "Epoch 247/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0783 - val_loss: 0.1722\n",
      "Epoch 248/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0658 - val_loss: 0.1782\n",
      "Epoch 249/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0749 - val_loss: 0.1652\n",
      "Epoch 250/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0641 - val_loss: 0.1815\n",
      "Epoch 251/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0743 - val_loss: 0.1747\n",
      "Epoch 252/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0733 - val_loss: 0.1684\n",
      "Epoch 253/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0719 - val_loss: 0.1667\n",
      "Epoch 254/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0625 - val_loss: 0.1646\n",
      "Epoch 255/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0691 - val_loss: 0.1746\n",
      "Epoch 256/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0723 - val_loss: 0.1655\n",
      "Epoch 257/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0711 - val_loss: 0.1648\n",
      "Epoch 258/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0732 - val_loss: 0.1619\n",
      "Epoch 259/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0635 - val_loss: 0.1737\n",
      "Epoch 260/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0654 - val_loss: 0.1752\n",
      "Epoch 261/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0759 - val_loss: 0.1917\n",
      "Epoch 262/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0680 - val_loss: 0.1735\n",
      "Epoch 263/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0810 - val_loss: 0.1722\n",
      "Epoch 264/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0619 - val_loss: 0.1786\n",
      "Epoch 265/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0714 - val_loss: 0.1705\n",
      "Epoch 266/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0618 - val_loss: 0.1634\n",
      "Epoch 267/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0591 - val_loss: 0.1781\n",
      "Epoch 268/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0654 - val_loss: 0.1668\n",
      "Epoch 269/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0654 - val_loss: 0.1663\n",
      "Epoch 270/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0652 - val_loss: 0.1744\n",
      "Epoch 271/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0669 - val_loss: 0.1721\n",
      "Epoch 272/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0729 - val_loss: 0.1813\n",
      "Epoch 273/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0726 - val_loss: 0.2024\n",
      "Epoch 274/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0699 - val_loss: 0.1842\n",
      "Epoch 275/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0685 - val_loss: 0.1690\n",
      "Epoch 276/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0637 - val_loss: 0.1681\n",
      "Epoch 277/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0705 - val_loss: 0.1634\n",
      "Epoch 278/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0605 - val_loss: 0.1653\n",
      "Epoch 279/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0630 - val_loss: 0.1695\n",
      "Epoch 280/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0653 - val_loss: 0.1668\n",
      "Epoch 281/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0675 - val_loss: 0.1622\n",
      "Epoch 282/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0609 - val_loss: 0.1836\n",
      "Epoch 283/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0600 - val_loss: 0.1651\n",
      "Epoch 284/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0681 - val_loss: 0.1647\n",
      "Epoch 285/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0739 - val_loss: 0.1690\n",
      "Epoch 286/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0583 - val_loss: 0.1655\n",
      "Epoch 287/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0676 - val_loss: 0.1628\n",
      "Epoch 288/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0828 - val_loss: 0.1933\n",
      "Epoch 289/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0743 - val_loss: 0.1656\n",
      "Epoch 290/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0562 - val_loss: 0.1662\n",
      "Epoch 291/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0887 - val_loss: 0.1861\n",
      "Epoch 292/1000\n",
      "934/934 [==============================] - 12765s 14s/step - loss: 0.0604 - val_loss: 0.1633\n",
      "Epoch 293/1000\n",
      "934/934 [==============================] - 26s 28ms/step - loss: 0.0624 - val_loss: 0.1699\n",
      "Epoch 294/1000\n",
      "934/934 [==============================] - 26s 28ms/step - loss: 0.0580 - val_loss: 0.1701\n",
      "Epoch 295/1000\n",
      "934/934 [==============================] - 26s 27ms/step - loss: 0.0632 - val_loss: 0.1884\n",
      "Epoch 296/1000\n",
      "934/934 [==============================] - 28s 30ms/step - loss: 0.0750 - val_loss: 0.1667\n",
      "Epoch 297/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0599 - val_loss: 0.1632\n",
      "Epoch 298/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0802 - val_loss: 0.1742\n",
      "Epoch 299/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0635 - val_loss: 0.1639\n",
      "Epoch 300/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0570 - val_loss: 0.1668\n",
      "Epoch 301/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0766 - val_loss: 0.1746\n",
      "Epoch 302/1000\n",
      "934/934 [==============================] - 29s 32ms/step - loss: 0.0615 - val_loss: 0.1691\n",
      "Epoch 303/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0704 - val_loss: 0.1661\n",
      "Epoch 304/1000\n",
      "934/934 [==============================] - 29s 32ms/step - loss: 0.0573 - val_loss: 0.1670\n",
      "Epoch 305/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0561 - val_loss: 0.1652\n",
      "Epoch 306/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0653 - val_loss: 0.1789\n",
      "Epoch 307/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0619 - val_loss: 0.1610\n",
      "Epoch 308/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0601 - val_loss: 0.1739\n",
      "Epoch 309/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0648 - val_loss: 0.1894\n",
      "Epoch 310/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0591 - val_loss: 0.1642\n",
      "Epoch 311/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0612 - val_loss: 0.1657\n",
      "Epoch 312/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0582 - val_loss: 0.1732\n",
      "Epoch 313/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0762 - val_loss: 0.1791\n",
      "Epoch 314/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0595 - val_loss: 0.1635\n",
      "Epoch 315/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0714 - val_loss: 0.1612\n",
      "Epoch 316/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0758 - val_loss: 0.1704\n",
      "Epoch 317/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0655 - val_loss: 0.1689\n",
      "Epoch 318/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0616 - val_loss: 0.1743\n",
      "Epoch 319/1000\n",
      "934/934 [==============================] - 29s 32ms/step - loss: 0.0596 - val_loss: 0.1625\n",
      "Epoch 320/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0595 - val_loss: 0.1842\n",
      "Epoch 321/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0661 - val_loss: 0.1629\n",
      "Epoch 322/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0620 - val_loss: 0.1590\n",
      "Epoch 323/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0622 - val_loss: 0.1678\n",
      "Epoch 324/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0620 - val_loss: 0.1661\n",
      "Epoch 325/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0651 - val_loss: 0.1694\n",
      "Epoch 326/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0683 - val_loss: 0.1603\n",
      "Epoch 327/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0654 - val_loss: 0.1629\n",
      "Epoch 328/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0562 - val_loss: 0.1634\n",
      "Epoch 329/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0605 - val_loss: 0.1663\n",
      "Epoch 330/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0596 - val_loss: 0.1874\n",
      "Epoch 331/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0685 - val_loss: 0.1583\n",
      "Epoch 332/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0671 - val_loss: 0.1607\n",
      "Epoch 333/1000\n",
      "934/934 [==============================] - 29s 32ms/step - loss: 0.0585 - val_loss: 0.1665\n",
      "Epoch 334/1000\n",
      "934/934 [==============================] - 29s 32ms/step - loss: 0.0569 - val_loss: 0.1695\n",
      "Epoch 335/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0593 - val_loss: 0.1652\n",
      "Epoch 336/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0604 - val_loss: 0.1657\n",
      "Epoch 337/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0626 - val_loss: 0.1635\n",
      "Epoch 338/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0647 - val_loss: 0.1638\n",
      "Epoch 339/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0556 - val_loss: 0.1613\n",
      "Epoch 340/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0626 - val_loss: 0.1789\n",
      "Epoch 341/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0563 - val_loss: 0.1620\n",
      "Epoch 342/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0721 - val_loss: 0.1602\n",
      "Epoch 343/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0686 - val_loss: 0.1690\n",
      "Epoch 344/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0613 - val_loss: 0.1659\n",
      "Epoch 345/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0642 - val_loss: 0.1629\n",
      "Epoch 346/1000\n",
      "934/934 [==============================] - 15290s 16s/step - loss: 0.0607 - val_loss: 0.1631\n",
      "Epoch 347/1000\n",
      "934/934 [==============================] - 26s 28ms/step - loss: 0.0653 - val_loss: 0.1776\n",
      "Epoch 348/1000\n",
      "934/934 [==============================] - 26s 27ms/step - loss: 0.0711 - val_loss: 0.1687\n",
      "Epoch 349/1000\n",
      "934/934 [==============================] - 26s 28ms/step - loss: 0.0582 - val_loss: 0.1709\n",
      "Epoch 350/1000\n",
      "934/934 [==============================] - 27s 29ms/step - loss: 0.0589 - val_loss: 0.1641\n",
      "Epoch 351/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0610 - val_loss: 0.1612\n",
      "Epoch 352/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0775 - val_loss: 0.1960\n",
      "Epoch 353/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0720 - val_loss: 0.1970\n",
      "Epoch 354/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0601 - val_loss: 0.1656\n",
      "Epoch 355/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0652 - val_loss: 0.1625\n",
      "Epoch 356/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0579 - val_loss: 0.1596\n",
      "Epoch 357/1000\n",
      "934/934 [==============================] - 22s 23ms/step - loss: 0.0537 - val_loss: 0.1657\n",
      "Epoch 358/1000\n",
      "934/934 [==============================] - 13s 14ms/step - loss: 0.0706 - val_loss: 0.1715\n",
      "Epoch 359/1000\n",
      "934/934 [==============================] - 13s 14ms/step - loss: 0.0693 - val_loss: 0.1881\n",
      "Epoch 360/1000\n",
      "934/934 [==============================] - 13s 14ms/step - loss: 0.0706 - val_loss: 0.1570\n",
      "Epoch 361/1000\n",
      "934/934 [==============================] - 13s 14ms/step - loss: 0.0664 - val_loss: 0.1660\n",
      "Epoch 362/1000\n",
      "934/934 [==============================] - 13s 14ms/step - loss: 0.0766 - val_loss: 0.1645\n",
      "Epoch 363/1000\n",
      "934/934 [==============================] - 13s 14ms/step - loss: 0.0603 - val_loss: 0.1609\n",
      "Epoch 364/1000\n",
      "934/934 [==============================] - 13s 14ms/step - loss: 0.0604 - val_loss: 0.1630\n",
      "Epoch 365/1000\n",
      "934/934 [==============================] - 13s 14ms/step - loss: 0.0678 - val_loss: 0.1661\n",
      "Epoch 366/1000\n",
      "934/934 [==============================] - 13s 14ms/step - loss: 0.0609 - val_loss: 0.1658\n",
      "Epoch 367/1000\n",
      "934/934 [==============================] - 23s 24ms/step - loss: 0.0540 - val_loss: 0.1619\n",
      "Epoch 368/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0623 - val_loss: 0.1698\n",
      "Epoch 369/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0605 - val_loss: 0.1701\n",
      "Epoch 370/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0503 - val_loss: 0.1731\n",
      "Epoch 371/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0604 - val_loss: 0.1630\n",
      "Epoch 372/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0573 - val_loss: 0.1746\n",
      "Epoch 373/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0687 - val_loss: 0.1613\n",
      "Epoch 374/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0636 - val_loss: 0.1936\n",
      "Epoch 375/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0567 - val_loss: 0.1596\n",
      "Epoch 376/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0593 - val_loss: 0.1699\n",
      "Epoch 377/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0645 - val_loss: 0.1755\n",
      "Epoch 378/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0601 - val_loss: 0.1602\n",
      "Epoch 379/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0555 - val_loss: 0.1715\n",
      "Epoch 380/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0603 - val_loss: 0.1914\n",
      "Epoch 381/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0763 - val_loss: 0.1705\n",
      "Epoch 382/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0674 - val_loss: 0.1647\n",
      "Epoch 383/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0671 - val_loss: 0.1640\n",
      "Epoch 384/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0659 - val_loss: 0.1670\n",
      "Epoch 385/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0711 - val_loss: 0.1889\n",
      "Epoch 386/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0751 - val_loss: 0.1724\n",
      "Epoch 387/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0551 - val_loss: 0.1618\n",
      "Epoch 388/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0668 - val_loss: 0.1614\n",
      "Epoch 389/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0639 - val_loss: 0.1639\n",
      "Epoch 390/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0679 - val_loss: 0.1618\n",
      "Epoch 391/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0538 - val_loss: 0.1640\n",
      "Epoch 392/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0649 - val_loss: 0.1627\n",
      "Epoch 393/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0552 - val_loss: 0.1602\n",
      "Epoch 394/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0540 - val_loss: 0.1605\n",
      "Epoch 395/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0587 - val_loss: 0.1645\n",
      "Epoch 396/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0660 - val_loss: 0.1609\n",
      "Epoch 397/1000\n",
      "934/934 [==============================] - 536s 573ms/step - loss: 0.0723 - val_loss: 0.1567\n",
      "Epoch 398/1000\n",
      "934/934 [==============================] - 26s 28ms/step - loss: 0.0674 - val_loss: 0.1781\n",
      "Epoch 399/1000\n",
      "934/934 [==============================] - 27s 28ms/step - loss: 0.0601 - val_loss: 0.1559\n",
      "Epoch 400/1000\n",
      "934/934 [==============================] - 26s 28ms/step - loss: 0.0591 - val_loss: 0.1609\n",
      "Epoch 401/1000\n",
      "934/934 [==============================] - 25s 27ms/step - loss: 0.0670 - val_loss: 0.1724\n",
      "Epoch 402/1000\n",
      "934/934 [==============================] - 27s 29ms/step - loss: 0.0652 - val_loss: 0.1649\n",
      "Epoch 403/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0738 - val_loss: 0.1597\n",
      "Epoch 404/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0587 - val_loss: 0.1683\n",
      "Epoch 405/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0593 - val_loss: 0.1818\n",
      "Epoch 406/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0601 - val_loss: 0.1996\n",
      "Epoch 407/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0668 - val_loss: 0.1656\n",
      "Epoch 408/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0646 - val_loss: 0.1618\n",
      "Epoch 409/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0532 - val_loss: 0.1625\n",
      "Epoch 410/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0646 - val_loss: 0.1736\n",
      "Epoch 411/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0711 - val_loss: 0.1864\n",
      "Epoch 412/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0669 - val_loss: 0.1687\n",
      "Epoch 413/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0646 - val_loss: 0.1640\n",
      "Epoch 414/1000\n",
      "934/934 [==============================] - 29s 32ms/step - loss: 0.0562 - val_loss: 0.1660\n",
      "Epoch 415/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0559 - val_loss: 0.1604\n",
      "Epoch 416/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0656 - val_loss: 0.1585\n",
      "Epoch 417/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0541 - val_loss: 0.1581\n",
      "Epoch 418/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0550 - val_loss: 0.1599\n",
      "Epoch 419/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0515 - val_loss: 0.1635\n",
      "Epoch 420/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0528 - val_loss: 0.2061\n",
      "Epoch 421/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0523 - val_loss: 0.1748\n",
      "Epoch 422/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0582 - val_loss: 0.1613\n",
      "Epoch 423/1000\n",
      "934/934 [==============================] - 29s 32ms/step - loss: 0.0558 - val_loss: 0.1610\n",
      "Epoch 424/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0624 - val_loss: 0.1746\n",
      "Epoch 425/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0561 - val_loss: 0.1770\n",
      "Epoch 426/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0574 - val_loss: 0.1820\n",
      "Epoch 427/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0607 - val_loss: 0.1715\n",
      "Epoch 428/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0580 - val_loss: 0.1614\n",
      "Epoch 429/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0582 - val_loss: 0.1640\n",
      "Epoch 430/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0595 - val_loss: 0.1613\n",
      "Epoch 431/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0559 - val_loss: 0.1586\n",
      "Epoch 432/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0611 - val_loss: 0.1653\n",
      "Epoch 433/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0546 - val_loss: 0.1662\n",
      "Epoch 434/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0621 - val_loss: 0.1631\n",
      "Epoch 435/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0537 - val_loss: 0.1684\n",
      "Epoch 436/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0696 - val_loss: 0.1660\n",
      "Epoch 437/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0719 - val_loss: 0.1948\n",
      "Epoch 438/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0622 - val_loss: 0.1614\n",
      "Epoch 439/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0708 - val_loss: 0.1572\n",
      "Epoch 440/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0512 - val_loss: 0.1674\n",
      "Epoch 441/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0573 - val_loss: 0.1618\n",
      "Epoch 442/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0663 - val_loss: 0.1590\n",
      "Epoch 443/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0510 - val_loss: 0.1568\n",
      "Epoch 444/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0592 - val_loss: 0.1586\n",
      "Epoch 445/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0610 - val_loss: 0.1797\n",
      "Epoch 446/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0627 - val_loss: 0.1659\n",
      "Epoch 447/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0560 - val_loss: 0.1577\n",
      "Epoch 448/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0487 - val_loss: 0.1617\n",
      "Epoch 449/1000\n",
      "934/934 [==============================] - 7354s 8s/step - loss: 0.0503 - val_loss: 0.1595\n",
      "Epoch 450/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0516 - val_loss: 0.1592\n",
      "Epoch 451/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0539 - val_loss: 0.1607\n",
      "Epoch 452/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0616 - val_loss: 0.1720\n",
      "Epoch 453/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0724 - val_loss: 0.1698\n",
      "Epoch 454/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0538 - val_loss: 0.1714\n",
      "Epoch 455/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0554 - val_loss: 0.1562\n",
      "Epoch 456/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0582 - val_loss: 0.1571\n",
      "Epoch 457/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0494 - val_loss: 0.1632\n",
      "Epoch 458/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0534 - val_loss: 0.1718\n",
      "Epoch 459/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0506 - val_loss: 0.1578\n",
      "Epoch 460/1000\n",
      "934/934 [==============================] - 40s 43ms/step - loss: 0.0677 - val_loss: 0.1657\n",
      "Epoch 461/1000\n",
      "934/934 [==============================] - 48s 51ms/step - loss: 0.0560 - val_loss: 0.1646\n",
      "Epoch 462/1000\n",
      "934/934 [==============================] - 47s 50ms/step - loss: 0.0497 - val_loss: 0.1806\n",
      "Epoch 463/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 47s 50ms/step - loss: 0.0592 - val_loss: 0.1636\n",
      "Epoch 464/1000\n",
      "934/934 [==============================] - 46s 49ms/step - loss: 0.0607 - val_loss: 0.1565\n",
      "Epoch 465/1000\n",
      "934/934 [==============================] - 47s 51ms/step - loss: 0.0582 - val_loss: 0.1598\n",
      "Epoch 466/1000\n",
      "934/934 [==============================] - 47s 51ms/step - loss: 0.0528 - val_loss: 0.1669\n",
      "Epoch 467/1000\n",
      "934/934 [==============================] - 47s 51ms/step - loss: 0.0575 - val_loss: 0.1895\n",
      "Epoch 468/1000\n",
      "934/934 [==============================] - 47s 51ms/step - loss: 0.0666 - val_loss: 0.1741\n",
      "Epoch 469/1000\n",
      "934/934 [==============================] - 47s 51ms/step - loss: 0.0622 - val_loss: 0.1637\n",
      "Epoch 470/1000\n",
      "934/934 [==============================] - 37s 40ms/step - loss: 0.0518 - val_loss: 0.1555\n",
      "Epoch 471/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0651 - val_loss: 0.1566\n",
      "Epoch 472/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0513 - val_loss: 0.1619\n",
      "Epoch 473/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0542 - val_loss: 0.1567\n",
      "Epoch 474/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0485 - val_loss: 0.1635\n",
      "Epoch 475/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0571 - val_loss: 0.1650\n",
      "Epoch 476/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0560 - val_loss: 0.1893\n",
      "Epoch 477/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0591 - val_loss: 0.1585\n",
      "Epoch 478/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0551 - val_loss: 0.1584\n",
      "Epoch 479/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0500 - val_loss: 0.1656\n",
      "Epoch 480/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0543 - val_loss: 0.1668\n",
      "Epoch 481/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0541 - val_loss: 0.1571\n",
      "Epoch 482/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0518 - val_loss: 0.1584\n",
      "Epoch 483/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0632 - val_loss: 0.1671\n",
      "Epoch 484/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0557 - val_loss: 0.1581\n",
      "Epoch 485/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0473 - val_loss: 0.1589\n",
      "Epoch 486/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0565 - val_loss: 0.1697\n",
      "Epoch 487/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0563 - val_loss: 0.1625\n",
      "Epoch 488/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0505 - val_loss: 0.1735\n",
      "Epoch 489/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0556 - val_loss: 0.1621\n",
      "Epoch 490/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0561 - val_loss: 0.1712\n",
      "Epoch 491/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0501 - val_loss: 0.1555\n",
      "Epoch 492/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0553 - val_loss: 0.1627\n",
      "Epoch 493/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0631 - val_loss: 0.1611\n",
      "Epoch 494/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0623 - val_loss: 0.1663\n",
      "Epoch 495/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0541 - val_loss: 0.1664\n",
      "Epoch 496/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0622 - val_loss: 0.1657\n",
      "Epoch 497/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0559 - val_loss: 0.1673\n",
      "Epoch 498/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0742 - val_loss: 0.1621\n",
      "Epoch 499/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0490 - val_loss: 0.1699\n",
      "Epoch 500/1000\n",
      "934/934 [==============================] - 4202s 4s/step - loss: 0.0610 - val_loss: 0.1559\n",
      "Epoch 501/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0585 - val_loss: 0.1652\n",
      "Epoch 502/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0626 - val_loss: 0.1545\n",
      "Epoch 503/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0475 - val_loss: 0.1656\n",
      "Epoch 504/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0578 - val_loss: 0.1636\n",
      "Epoch 505/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0506 - val_loss: 0.1609\n",
      "Epoch 506/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0520 - val_loss: 0.1586\n",
      "Epoch 507/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0542 - val_loss: 0.1648\n",
      "Epoch 508/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0543 - val_loss: 0.1607\n",
      "Epoch 509/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0635 - val_loss: 0.1584\n",
      "Epoch 510/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0523 - val_loss: 0.1637\n",
      "Epoch 511/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0490 - val_loss: 0.1679\n",
      "Epoch 512/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0631 - val_loss: 0.1751\n",
      "Epoch 513/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0537 - val_loss: 0.1923\n",
      "Epoch 514/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0560 - val_loss: 0.1570\n",
      "Epoch 515/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0569 - val_loss: 0.1575\n",
      "Epoch 516/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0548 - val_loss: 0.1569\n",
      "Epoch 517/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0458 - val_loss: 0.1565\n",
      "Epoch 518/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0605 - val_loss: 0.1598\n",
      "Epoch 519/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0541 - val_loss: 0.1610\n",
      "Epoch 520/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0524 - val_loss: 0.1568\n",
      "Epoch 521/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0457 - val_loss: 0.1637\n",
      "Epoch 522/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0702 - val_loss: 0.1572\n",
      "Epoch 523/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0583 - val_loss: 0.1598\n",
      "Epoch 524/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0538 - val_loss: 0.1586\n",
      "Epoch 525/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0513 - val_loss: 0.1609\n",
      "Epoch 526/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0603 - val_loss: 0.1633\n",
      "Epoch 527/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0569 - val_loss: 0.1738\n",
      "Epoch 528/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0499 - val_loss: 0.1581\n",
      "Epoch 529/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0588 - val_loss: 0.1548\n",
      "Epoch 530/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0551 - val_loss: 0.1843\n",
      "Epoch 531/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0581 - val_loss: 0.1739\n",
      "Epoch 532/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0581 - val_loss: 0.1576\n",
      "Epoch 533/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0518 - val_loss: 0.1572\n",
      "Epoch 534/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0571 - val_loss: 0.1658\n",
      "Epoch 535/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0547 - val_loss: 0.1729\n",
      "Epoch 536/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0539 - val_loss: 0.1602\n",
      "Epoch 537/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0457 - val_loss: 0.1577\n",
      "Epoch 538/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0570 - val_loss: 0.1607\n",
      "Epoch 539/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0532 - val_loss: 0.1591\n",
      "Epoch 540/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0488 - val_loss: 0.1751\n",
      "Epoch 541/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0577 - val_loss: 0.1763\n",
      "Epoch 542/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0496 - val_loss: 0.1604\n",
      "Epoch 543/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0507 - val_loss: 0.1602\n",
      "Epoch 544/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0531 - val_loss: 0.1651\n",
      "Epoch 545/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0512 - val_loss: 0.1750\n",
      "Epoch 546/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0583 - val_loss: 0.1572\n",
      "Epoch 547/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0560 - val_loss: 0.1569\n",
      "Epoch 548/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0552 - val_loss: 0.1552\n",
      "Epoch 549/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0467 - val_loss: 0.1562\n",
      "Epoch 550/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0498 - val_loss: 0.1666\n",
      "Epoch 551/1000\n",
      "934/934 [==============================] - 5715s 6s/step - loss: 0.0539 - val_loss: 0.1638\n",
      "Epoch 552/1000\n",
      "934/934 [==============================] - 27s 29ms/step - loss: 0.0508 - val_loss: 0.1594\n",
      "Epoch 553/1000\n",
      "934/934 [==============================] - 28s 30ms/step - loss: 0.0527 - val_loss: 0.1562\n",
      "Epoch 554/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0537 - val_loss: 0.1567\n",
      "Epoch 555/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0528 - val_loss: 0.1568\n",
      "Epoch 556/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0649 - val_loss: 0.1664\n",
      "Epoch 557/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0586 - val_loss: 0.1589\n",
      "Epoch 558/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0556 - val_loss: 0.1557\n",
      "Epoch 559/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0491 - val_loss: 0.1717\n",
      "Epoch 560/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0538 - val_loss: 0.1857\n",
      "Epoch 561/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0633 - val_loss: 0.1604\n",
      "Epoch 562/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0523 - val_loss: 0.1676\n",
      "Epoch 563/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0579 - val_loss: 0.1709\n",
      "Epoch 564/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0532 - val_loss: 0.1580\n",
      "Epoch 565/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0502 - val_loss: 0.1655\n",
      "Epoch 566/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0639 - val_loss: 0.1703\n",
      "Epoch 567/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0501 - val_loss: 0.1587\n",
      "Epoch 568/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0497 - val_loss: 0.1545\n",
      "Epoch 569/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0473 - val_loss: 0.1821\n",
      "Epoch 570/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0635 - val_loss: 0.1601\n",
      "Epoch 571/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0553 - val_loss: 0.1565\n",
      "Epoch 572/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0525 - val_loss: 0.1606\n",
      "Epoch 573/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0431 - val_loss: 0.1624\n",
      "Epoch 574/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0495 - val_loss: 0.1575\n",
      "Epoch 575/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0608 - val_loss: 0.2063\n",
      "Epoch 576/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0655 - val_loss: 0.1639\n",
      "Epoch 577/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0606 - val_loss: 0.1651\n",
      "Epoch 578/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0527 - val_loss: 0.1613\n",
      "Epoch 579/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0491 - val_loss: 0.1684\n",
      "Epoch 580/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0576 - val_loss: 0.1690\n",
      "Epoch 581/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0524 - val_loss: 0.1561\n",
      "Epoch 582/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0486 - val_loss: 0.1626\n",
      "Epoch 583/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0501 - val_loss: 0.1604\n",
      "Epoch 584/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0498 - val_loss: 0.1579\n",
      "Epoch 585/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0595 - val_loss: 0.1581\n",
      "Epoch 586/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0471 - val_loss: 0.1686\n",
      "Epoch 587/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0507 - val_loss: 0.1739\n",
      "Epoch 588/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0528 - val_loss: 0.1598\n",
      "Epoch 589/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0514 - val_loss: 0.1606\n",
      "Epoch 590/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0469 - val_loss: 0.1571\n",
      "Epoch 591/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0498 - val_loss: 0.1572\n",
      "Epoch 592/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0539 - val_loss: 0.1593\n",
      "Epoch 593/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0473 - val_loss: 0.1648\n",
      "Epoch 594/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0553 - val_loss: 0.1580\n",
      "Epoch 595/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0513 - val_loss: 0.1609\n",
      "Epoch 596/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0601 - val_loss: 0.1558\n",
      "Epoch 597/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0503 - val_loss: 0.1631\n",
      "Epoch 598/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0532 - val_loss: 0.1589\n",
      "Epoch 599/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0521 - val_loss: 0.1572\n",
      "Epoch 600/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0522 - val_loss: 0.1613\n",
      "Epoch 601/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0501 - val_loss: 0.1592\n",
      "Epoch 602/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0516 - val_loss: 0.1693\n",
      "Epoch 603/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0592 - val_loss: 0.1600\n",
      "Epoch 604/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0565 - val_loss: 0.1665\n",
      "Epoch 605/1000\n",
      "934/934 [==============================] - 817s 874ms/step - loss: 0.0523 - val_loss: 0.1600\n",
      "Epoch 606/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0508 - val_loss: 0.1561\n",
      "Epoch 607/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0560 - val_loss: 0.1712\n",
      "Epoch 608/1000\n",
      "934/934 [==============================] - 32s 35ms/step - loss: 0.0470 - val_loss: 0.1575\n",
      "Epoch 609/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0538 - val_loss: 0.1614\n",
      "Epoch 610/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0473 - val_loss: 0.1552\n",
      "Epoch 611/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0450 - val_loss: 0.1586\n",
      "Epoch 612/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0455 - val_loss: 0.1593\n",
      "Epoch 613/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0479 - val_loss: 0.1577\n",
      "Epoch 614/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0592 - val_loss: 0.1577\n",
      "Epoch 615/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0572 - val_loss: 0.1581\n",
      "Epoch 616/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0522 - val_loss: 0.2012\n",
      "Epoch 617/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0597 - val_loss: 0.1617\n",
      "Epoch 618/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0453 - val_loss: 0.1612\n",
      "Epoch 619/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0459 - val_loss: 0.1652\n",
      "Epoch 620/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0466 - val_loss: 0.1598\n",
      "Epoch 621/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0509 - val_loss: 0.1567\n",
      "Epoch 622/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0524 - val_loss: 0.1565\n",
      "Epoch 623/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0631 - val_loss: 0.1664\n",
      "Epoch 624/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0442 - val_loss: 0.1544\n",
      "Epoch 625/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0490 - val_loss: 0.1609\n",
      "Epoch 626/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0458 - val_loss: 0.1605\n",
      "Epoch 627/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0459 - val_loss: 0.1555\n",
      "Epoch 628/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0602 - val_loss: 0.1572\n",
      "Epoch 629/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0632 - val_loss: 0.1556\n",
      "Epoch 630/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0456 - val_loss: 0.1544\n",
      "Epoch 631/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0475 - val_loss: 0.1577\n",
      "Epoch 632/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0574 - val_loss: 0.1574\n",
      "Epoch 633/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0429 - val_loss: 0.1578\n",
      "Epoch 634/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0501 - val_loss: 0.1552\n",
      "Epoch 635/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0511 - val_loss: 0.1572\n",
      "Epoch 636/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0576 - val_loss: 0.1585\n",
      "Epoch 637/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0467 - val_loss: 0.1557\n",
      "Epoch 638/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0454 - val_loss: 0.1579\n",
      "Epoch 639/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0566 - val_loss: 0.1611\n",
      "Epoch 640/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0573 - val_loss: 0.1619\n",
      "Epoch 641/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0518 - val_loss: 0.1616\n",
      "Epoch 642/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0529 - val_loss: 0.1772\n",
      "Epoch 643/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0485 - val_loss: 0.1605\n",
      "Epoch 644/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0467 - val_loss: 0.1815\n",
      "Epoch 645/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0520 - val_loss: 0.1640\n",
      "Epoch 646/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0502 - val_loss: 0.1580\n",
      "Epoch 647/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0502 - val_loss: 0.1660\n",
      "Epoch 648/1000\n",
      "934/934 [==============================] - 29s 32ms/step - loss: 0.0441 - val_loss: 0.1570\n",
      "Epoch 649/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0459 - val_loss: 0.1562\n",
      "Epoch 650/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0503 - val_loss: 0.1607\n",
      "Epoch 651/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0478 - val_loss: 0.1766\n",
      "Epoch 652/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0461 - val_loss: 0.1622\n",
      "Epoch 653/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0515 - val_loss: 0.1813\n",
      "Epoch 654/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0562 - val_loss: 0.1819\n",
      "Epoch 655/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0495 - val_loss: 0.1669\n",
      "Epoch 656/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0566 - val_loss: 0.1566\n",
      "Epoch 657/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0528 - val_loss: 0.1606\n",
      "Epoch 658/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0428 - val_loss: 0.1555\n",
      "Epoch 659/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0596 - val_loss: 0.1618\n",
      "Epoch 660/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0542 - val_loss: 0.1562\n",
      "Epoch 661/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0473 - val_loss: 0.1588\n",
      "Epoch 662/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0473 - val_loss: 0.1555\n",
      "Epoch 663/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0568 - val_loss: 0.1802\n",
      "Epoch 664/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0585 - val_loss: 0.1675\n",
      "Epoch 665/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0487 - val_loss: 0.1595\n",
      "Epoch 666/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0481 - val_loss: 0.1532\n",
      "Epoch 667/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0501 - val_loss: 0.1585\n",
      "Epoch 668/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0531 - val_loss: 0.1558\n",
      "Epoch 669/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0594 - val_loss: 0.1653\n",
      "Epoch 670/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0468 - val_loss: 0.1545\n",
      "Epoch 671/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0608 - val_loss: 0.1588\n",
      "Epoch 672/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0559 - val_loss: 0.1610\n",
      "Epoch 673/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0600 - val_loss: 0.1638\n",
      "Epoch 674/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0516 - val_loss: 0.1633\n",
      "Epoch 675/1000\n",
      "934/934 [==============================] - 213s 228ms/step - loss: 0.0590 - val_loss: 0.1556\n",
      "Epoch 676/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0458 - val_loss: 0.1594\n",
      "Epoch 677/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0442 - val_loss: 0.1596\n",
      "Epoch 678/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0488 - val_loss: 0.1562\n",
      "Epoch 679/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0387 - val_loss: 0.1596\n",
      "Epoch 680/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0597 - val_loss: 0.1683\n",
      "Epoch 681/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0478 - val_loss: 0.1691\n",
      "Epoch 682/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0478 - val_loss: 0.1558\n",
      "Epoch 683/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0475 - val_loss: 0.1626\n",
      "Epoch 684/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0549 - val_loss: 0.1715\n",
      "Epoch 685/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0466 - val_loss: 0.1573\n",
      "Epoch 686/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0457 - val_loss: 0.1640\n",
      "Epoch 687/1000\n",
      "934/934 [==============================] - 32s 35ms/step - loss: 0.0550 - val_loss: 0.1599\n",
      "Epoch 688/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0569 - val_loss: 0.1565\n",
      "Epoch 689/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0455 - val_loss: 0.1610\n",
      "Epoch 690/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0549 - val_loss: 0.1554\n",
      "Epoch 691/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0463 - val_loss: 0.1942\n",
      "Epoch 692/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0609 - val_loss: 0.1668\n",
      "Epoch 693/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0483 - val_loss: 0.1597\n",
      "Epoch 694/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0486 - val_loss: 0.1575\n",
      "Epoch 695/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0519 - val_loss: 0.1604\n",
      "Epoch 696/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0463 - val_loss: 0.1535\n",
      "Epoch 697/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0441 - val_loss: 0.1554\n",
      "Epoch 698/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0500 - val_loss: 0.1615\n",
      "Epoch 699/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0483 - val_loss: 0.1556\n",
      "Epoch 700/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0477 - val_loss: 0.1561\n",
      "Epoch 701/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0460 - val_loss: 0.1580\n",
      "Epoch 702/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0566 - val_loss: 0.1626\n",
      "Epoch 703/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0493 - val_loss: 0.1616\n",
      "Epoch 704/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0533 - val_loss: 0.1647\n",
      "Epoch 705/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0504 - val_loss: 0.1592\n",
      "Epoch 706/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0538 - val_loss: 0.1592\n",
      "Epoch 707/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0460 - val_loss: 0.1551\n",
      "Epoch 708/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0471 - val_loss: 0.1600\n",
      "Epoch 709/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0530 - val_loss: 0.1639\n",
      "Epoch 710/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0433 - val_loss: 0.1839\n",
      "Epoch 711/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0531 - val_loss: 0.1601\n",
      "Epoch 712/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0537 - val_loss: 0.1653\n",
      "Epoch 713/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0582 - val_loss: 0.1582\n",
      "Epoch 714/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0776 - val_loss: 0.1688\n",
      "Epoch 715/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0436 - val_loss: 0.1757\n",
      "Epoch 716/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0509 - val_loss: 0.1562\n",
      "Epoch 717/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0504 - val_loss: 0.1706\n",
      "Epoch 718/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0457 - val_loss: 0.1552\n",
      "Epoch 719/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0449 - val_loss: 0.1585\n",
      "Epoch 720/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0521 - val_loss: 0.1567\n",
      "Epoch 721/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0457 - val_loss: 0.1668\n",
      "Epoch 722/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0471 - val_loss: 0.1674\n",
      "Epoch 723/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0529 - val_loss: 0.1586\n",
      "Epoch 724/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0444 - val_loss: 0.1652\n",
      "Epoch 725/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0556 - val_loss: 0.1582\n",
      "Epoch 726/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0514 - val_loss: 0.1699\n",
      "Epoch 727/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0575 - val_loss: 0.1790\n",
      "Epoch 728/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0472 - val_loss: 0.1548\n",
      "Epoch 729/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0565 - val_loss: 0.1574\n",
      "Epoch 730/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0605 - val_loss: 0.1728\n",
      "Epoch 731/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0502 - val_loss: 0.1545\n",
      "Epoch 732/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0464 - val_loss: 0.1610\n",
      "Epoch 733/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0548 - val_loss: 0.1580\n",
      "Epoch 734/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0519 - val_loss: 0.1652\n",
      "Epoch 735/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0667 - val_loss: 0.1696\n",
      "Epoch 736/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0495 - val_loss: 0.2075\n",
      "Epoch 737/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0532 - val_loss: 0.1621\n",
      "Epoch 738/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0487 - val_loss: 0.1743\n",
      "Epoch 739/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0478 - val_loss: 0.1581\n",
      "Epoch 740/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0497 - val_loss: 0.1561\n",
      "Epoch 741/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0422 - val_loss: 0.1612\n",
      "Epoch 742/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0542 - val_loss: 0.1624\n",
      "Epoch 743/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0552 - val_loss: 0.1528\n",
      "Epoch 744/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0494 - val_loss: 0.1580\n",
      "Epoch 745/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0463 - val_loss: 0.1538\n",
      "Epoch 746/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0468 - val_loss: 0.1707\n",
      "Epoch 747/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0420 - val_loss: 0.1599\n",
      "Epoch 748/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0456 - val_loss: 0.1667\n",
      "Epoch 749/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0430 - val_loss: 0.1569\n",
      "Epoch 750/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0596 - val_loss: 0.1546\n",
      "Epoch 751/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0466 - val_loss: 0.1581\n",
      "Epoch 752/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0573 - val_loss: 0.1570\n",
      "Epoch 753/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0494 - val_loss: 0.1813\n",
      "Epoch 754/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0544 - val_loss: 0.1583\n",
      "Epoch 755/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0554 - val_loss: 0.1563\n",
      "Epoch 756/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0501 - val_loss: 0.1594\n",
      "Epoch 757/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0431 - val_loss: 0.1574\n",
      "Epoch 758/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0428 - val_loss: 0.1627\n",
      "Epoch 759/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0503 - val_loss: 0.1610\n",
      "Epoch 760/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0613 - val_loss: 0.1785\n",
      "Epoch 761/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0558 - val_loss: 0.1632\n",
      "Epoch 762/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0503 - val_loss: 0.1577\n",
      "Epoch 763/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0583 - val_loss: 0.1570\n",
      "Epoch 764/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0468 - val_loss: 0.1766\n",
      "Epoch 765/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0561 - val_loss: 0.1745\n",
      "Epoch 766/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0451 - val_loss: 0.1640\n",
      "Epoch 767/1000\n",
      "934/934 [==============================] - 37s 40ms/step - loss: 0.0532 - val_loss: 0.1683\n",
      "Epoch 768/1000\n",
      "934/934 [==============================] - 36s 39ms/step - loss: 0.0483 - val_loss: 0.1559\n",
      "Epoch 769/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.0589 - val_loss: 0.1631\n",
      "Epoch 770/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0442 - val_loss: 0.1609\n",
      "Epoch 771/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0405 - val_loss: 0.1635\n",
      "Epoch 772/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0483 - val_loss: 0.1616\n",
      "Epoch 773/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0382 - val_loss: 0.1630\n",
      "Epoch 774/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0470 - val_loss: 0.1611\n",
      "Epoch 775/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0453 - val_loss: 0.1547\n",
      "Epoch 776/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0464 - val_loss: 0.1648\n",
      "Epoch 777/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0438 - val_loss: 0.1696\n",
      "Epoch 778/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0523 - val_loss: 0.1619\n",
      "Epoch 779/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0505 - val_loss: 0.1608\n",
      "Epoch 780/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0500 - val_loss: 0.1583\n",
      "Epoch 781/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0452 - val_loss: 0.1636\n",
      "Epoch 782/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0462 - val_loss: 0.1615\n",
      "Epoch 783/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0429 - val_loss: 0.1585\n",
      "Epoch 784/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0656 - val_loss: 0.1654\n",
      "Epoch 785/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0470 - val_loss: 0.1601\n",
      "Epoch 786/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0453 - val_loss: 0.1550\n",
      "Epoch 787/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0400 - val_loss: 0.1565\n",
      "Epoch 788/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0420 - val_loss: 0.1574\n",
      "Epoch 789/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0494 - val_loss: 0.1603\n",
      "Epoch 790/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0402 - val_loss: 0.1559\n",
      "Epoch 791/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0695 - val_loss: 0.1576\n",
      "Epoch 792/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0589 - val_loss: 0.1600\n",
      "Epoch 793/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0486 - val_loss: 0.1762\n",
      "Epoch 794/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0480 - val_loss: 0.1559\n",
      "Epoch 795/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0452 - val_loss: 0.1606\n",
      "Epoch 796/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0464 - val_loss: 0.1579\n",
      "Epoch 797/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0533 - val_loss: 0.1592\n",
      "Epoch 798/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0524 - val_loss: 0.1599\n",
      "Epoch 799/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0481 - val_loss: 0.1839\n",
      "Epoch 800/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0529 - val_loss: 0.1647\n",
      "Epoch 801/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0479 - val_loss: 0.1621\n",
      "Epoch 802/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0484 - val_loss: 0.1568\n",
      "Epoch 803/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0468 - val_loss: 0.1635\n",
      "Epoch 804/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0459 - val_loss: 0.1561\n",
      "Epoch 805/1000\n",
      "934/934 [==============================] - 35s 37ms/step - loss: 0.0530 - val_loss: 0.1645\n",
      "Epoch 806/1000\n",
      "934/934 [==============================] - 36s 39ms/step - loss: 0.0484 - val_loss: 0.1583\n",
      "Epoch 807/1000\n",
      "934/934 [==============================] - 34s 37ms/step - loss: 0.0461 - val_loss: 0.1704\n",
      "Epoch 808/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0479 - val_loss: 0.1683\n",
      "Epoch 809/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0450 - val_loss: 0.1624\n",
      "Epoch 810/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0504 - val_loss: 0.1637\n",
      "Epoch 811/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0486 - val_loss: 0.1587\n",
      "Epoch 812/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0443 - val_loss: 0.1754\n",
      "Epoch 813/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0464 - val_loss: 0.1581\n",
      "Epoch 814/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0468 - val_loss: 0.1808\n",
      "Epoch 815/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0462 - val_loss: 0.1569\n",
      "Epoch 816/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0508 - val_loss: 0.1600\n",
      "Epoch 817/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0475 - val_loss: 0.1537\n",
      "Epoch 818/1000\n",
      "934/934 [==============================] - 35s 37ms/step - loss: 0.0422 - val_loss: 0.1559\n",
      "Epoch 819/1000\n",
      "934/934 [==============================] - 35s 38ms/step - loss: 0.0446 - val_loss: 0.1661\n",
      "Epoch 820/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0425 - val_loss: 0.1576\n",
      "Epoch 821/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0466 - val_loss: 0.1630\n",
      "Epoch 822/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0549 - val_loss: 0.1648\n",
      "Epoch 823/1000\n",
      "934/934 [==============================] - 39s 42ms/step - loss: 0.0491 - val_loss: 0.1634\n",
      "Epoch 824/1000\n",
      "934/934 [==============================] - 36s 38ms/step - loss: 0.0490 - val_loss: 0.1598\n",
      "Epoch 825/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0460 - val_loss: 0.1689\n",
      "Epoch 826/1000\n",
      "934/934 [==============================] - 34s 37ms/step - loss: 0.0444 - val_loss: 0.1599\n",
      "Epoch 827/1000\n",
      "934/934 [==============================] - 35s 37ms/step - loss: 0.0465 - val_loss: 0.1591\n",
      "Epoch 828/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0446 - val_loss: 0.1565\n",
      "Epoch 829/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0436 - val_loss: 0.1704\n",
      "Epoch 830/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0443 - val_loss: 0.1615\n",
      "Epoch 831/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0419 - val_loss: 0.1644\n",
      "Epoch 832/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0508 - val_loss: 0.1588\n",
      "Epoch 833/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0476 - val_loss: 0.1627\n",
      "Epoch 834/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0482 - val_loss: 0.1715\n",
      "Epoch 835/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0462 - val_loss: 0.1581\n",
      "Epoch 836/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0518 - val_loss: 0.1612\n",
      "Epoch 837/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0459 - val_loss: 0.1665\n",
      "Epoch 838/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0448 - val_loss: 0.1599\n",
      "Epoch 839/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0549 - val_loss: 0.1627\n",
      "Epoch 840/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0480 - val_loss: 0.1661\n",
      "Epoch 841/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0423 - val_loss: 0.1576\n",
      "Epoch 842/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0608 - val_loss: 0.1967\n",
      "Epoch 843/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0546 - val_loss: 0.1596\n",
      "Epoch 844/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0520 - val_loss: 0.1719\n",
      "Epoch 845/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0519 - val_loss: 0.1640\n",
      "Epoch 846/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0562 - val_loss: 0.1548\n",
      "Epoch 847/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0419 - val_loss: 0.1574\n",
      "Epoch 848/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0448 - val_loss: 0.1571\n",
      "Epoch 849/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0518 - val_loss: 0.1619\n",
      "Epoch 850/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0419 - val_loss: 0.1658\n",
      "Epoch 851/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0419 - val_loss: 0.1579\n",
      "Epoch 852/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0410 - val_loss: 0.1668\n",
      "Epoch 853/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0572 - val_loss: 0.1600\n",
      "Epoch 854/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0373 - val_loss: 0.1609\n",
      "Epoch 855/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0395 - val_loss: 0.1633\n",
      "Epoch 856/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0577 - val_loss: 0.1925\n",
      "Epoch 857/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0431 - val_loss: 0.1628\n",
      "Epoch 858/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0616 - val_loss: 0.1569\n",
      "Epoch 859/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0397 - val_loss: 0.1631\n",
      "Epoch 860/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0448 - val_loss: 0.1557\n",
      "Epoch 861/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0395 - val_loss: 0.1628\n",
      "Epoch 862/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0427 - val_loss: 0.1605\n",
      "Epoch 863/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0442 - val_loss: 0.1701\n",
      "Epoch 864/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0450 - val_loss: 0.1553\n",
      "Epoch 865/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0429 - val_loss: 0.1607\n",
      "Epoch 866/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.0459 - val_loss: 0.1619\n",
      "Epoch 867/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0442 - val_loss: 0.1596\n",
      "Epoch 868/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0412 - val_loss: 0.1592\n",
      "Epoch 869/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0470 - val_loss: 0.1552\n",
      "Epoch 870/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0407 - val_loss: 0.1682\n",
      "Epoch 871/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0584 - val_loss: 0.1640\n",
      "Epoch 872/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0445 - val_loss: 0.1586\n",
      "Epoch 873/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0405 - val_loss: 0.1596\n",
      "Epoch 874/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0485 - val_loss: 0.1726\n",
      "Epoch 875/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0515 - val_loss: 0.1583\n",
      "Epoch 876/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0434 - val_loss: 0.1642\n",
      "Epoch 877/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0470 - val_loss: 0.1552\n",
      "Epoch 878/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0525 - val_loss: 0.1551\n",
      "Epoch 879/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0464 - val_loss: 0.1566\n",
      "Epoch 880/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0454 - val_loss: 0.1627\n",
      "Epoch 881/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0383 - val_loss: 0.1557\n",
      "Epoch 882/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0485 - val_loss: 0.1633\n",
      "Epoch 883/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0414 - val_loss: 0.1588\n",
      "Epoch 884/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0467 - val_loss: 0.1602\n",
      "Epoch 885/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0511 - val_loss: 0.1591\n",
      "Epoch 886/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0400 - val_loss: 0.1576\n",
      "Epoch 887/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0498 - val_loss: 0.1645\n",
      "Epoch 888/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0423 - val_loss: 0.1547\n",
      "Epoch 889/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0413 - val_loss: 0.1645\n",
      "Epoch 890/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0537 - val_loss: 0.1601\n",
      "Epoch 891/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0517 - val_loss: 0.1637\n",
      "Epoch 892/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0429 - val_loss: 0.1609\n",
      "Epoch 893/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0493 - val_loss: 0.1575\n",
      "Epoch 894/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0419 - val_loss: 0.1580\n",
      "Epoch 895/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0561 - val_loss: 0.1628\n",
      "Epoch 896/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0457 - val_loss: 0.1600\n",
      "Epoch 897/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0404 - val_loss: 0.1668\n",
      "Epoch 898/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0569 - val_loss: 0.1626\n",
      "Epoch 899/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0406 - val_loss: 0.1668\n",
      "Epoch 900/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0586 - val_loss: 0.1586\n",
      "Epoch 901/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0504 - val_loss: 0.1597\n",
      "Epoch 902/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0381 - val_loss: 0.1587\n",
      "Epoch 903/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0428 - val_loss: 0.1661\n",
      "Epoch 904/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0425 - val_loss: 0.1667\n",
      "Epoch 905/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0417 - val_loss: 0.1612\n",
      "Epoch 906/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0390 - val_loss: 0.1597\n",
      "Epoch 907/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0428 - val_loss: 0.1569\n",
      "Epoch 908/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0397 - val_loss: 0.1601\n",
      "Epoch 909/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0451 - val_loss: 0.1560\n",
      "Epoch 910/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0447 - val_loss: 0.1639\n",
      "Epoch 911/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0502 - val_loss: 0.1592\n",
      "Epoch 912/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0446 - val_loss: 0.1577\n",
      "Epoch 913/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0422 - val_loss: 0.1636\n",
      "Epoch 914/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0508 - val_loss: 0.1720\n",
      "Epoch 915/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0448 - val_loss: 0.1592\n",
      "Epoch 916/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0370 - val_loss: 0.1630\n",
      "Epoch 917/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0459 - val_loss: 0.1620\n",
      "Epoch 918/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0427 - val_loss: 0.1623\n",
      "Epoch 919/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0441 - val_loss: 0.1533\n",
      "Epoch 920/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0563 - val_loss: 0.1619\n",
      "Epoch 921/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0404 - val_loss: 0.1615\n",
      "Epoch 922/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0465 - val_loss: 0.1587\n",
      "Epoch 923/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0440 - val_loss: 0.1558\n",
      "Epoch 924/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0390 - val_loss: 0.1633\n",
      "Epoch 925/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0483 - val_loss: 0.1627\n",
      "Epoch 926/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0525 - val_loss: 0.1791\n",
      "Epoch 927/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0459 - val_loss: 0.1639\n",
      "Epoch 928/1000\n",
      "934/934 [==============================] - 37s 39ms/step - loss: 0.0410 - val_loss: 0.1676\n",
      "Epoch 929/1000\n",
      "934/934 [==============================] - 34s 37ms/step - loss: 0.0416 - val_loss: 0.1670\n",
      "Epoch 930/1000\n",
      "934/934 [==============================] - 41s 44ms/step - loss: 0.0421 - val_loss: 0.1624\n",
      "Epoch 931/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0441 - val_loss: 0.1590\n",
      "Epoch 932/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0438 - val_loss: 0.1616\n",
      "Epoch 933/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0483 - val_loss: 0.1603\n",
      "Epoch 934/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0466 - val_loss: 0.1597\n",
      "Epoch 935/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0445 - val_loss: 0.1648\n",
      "Epoch 936/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0418 - val_loss: 0.1660\n",
      "Epoch 937/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0625 - val_loss: 0.1589\n",
      "Epoch 938/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0425 - val_loss: 0.1621\n",
      "Epoch 939/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0489 - val_loss: 0.1604\n",
      "Epoch 940/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0522 - val_loss: 0.1611\n",
      "Epoch 941/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0502 - val_loss: 0.1630\n",
      "Epoch 942/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0416 - val_loss: 0.1614\n",
      "Epoch 943/1000\n",
      "934/934 [==============================] - 33s 36ms/step - loss: 0.0408 - val_loss: 0.1588\n",
      "Epoch 944/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0573 - val_loss: 0.1707\n",
      "Epoch 945/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0501 - val_loss: 0.1604\n",
      "Epoch 946/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0524 - val_loss: 0.1610\n",
      "Epoch 947/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0467 - val_loss: 0.1594\n",
      "Epoch 948/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0497 - val_loss: 0.1734\n",
      "Epoch 949/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0508 - val_loss: 0.1580\n",
      "Epoch 950/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0450 - val_loss: 0.1719\n",
      "Epoch 951/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0396 - val_loss: 0.1628\n",
      "Epoch 952/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0450 - val_loss: 0.1596\n",
      "Epoch 953/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0460 - val_loss: 0.1630\n",
      "Epoch 954/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0404 - val_loss: 0.1668\n",
      "Epoch 955/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0422 - val_loss: 0.1605\n",
      "Epoch 956/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0485 - val_loss: 0.1627\n",
      "Epoch 957/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0440 - val_loss: 0.1578\n",
      "Epoch 958/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0402 - val_loss: 0.1578\n",
      "Epoch 959/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0392 - val_loss: 0.1620\n",
      "Epoch 960/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0422 - val_loss: 0.1564\n",
      "Epoch 961/1000\n",
      "934/934 [==============================] - 30s 33ms/step - loss: 0.0422 - val_loss: 0.1532\n",
      "Epoch 962/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0393 - val_loss: 0.1580\n",
      "Epoch 963/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0422 - val_loss: 0.1586\n",
      "Epoch 964/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0370 - val_loss: 0.1600\n",
      "Epoch 965/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0468 - val_loss: 0.1593\n",
      "Epoch 966/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0398 - val_loss: 0.1649\n",
      "Epoch 967/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0418 - val_loss: 0.1626\n",
      "Epoch 968/1000\n",
      "934/934 [==============================] - 32s 35ms/step - loss: 0.0398 - val_loss: 0.1599\n",
      "Epoch 969/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0436 - val_loss: 0.1644\n",
      "Epoch 970/1000\n",
      "934/934 [==============================] - 34s 36ms/step - loss: 0.0412 - val_loss: 0.1649\n",
      "Epoch 971/1000\n",
      "934/934 [==============================] - 36s 38ms/step - loss: 0.0421 - val_loss: 0.1672\n",
      "Epoch 972/1000\n",
      "934/934 [==============================] - 32s 34ms/step - loss: 0.0512 - val_loss: 0.1581\n",
      "Epoch 973/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0398 - val_loss: 0.1588\n",
      "Epoch 974/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0379 - val_loss: 0.1598\n",
      "Epoch 975/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0449 - val_loss: 0.1568\n",
      "Epoch 976/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0573 - val_loss: 0.1710\n",
      "Epoch 977/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0449 - val_loss: 0.1624\n",
      "Epoch 978/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0710 - val_loss: 0.1629\n",
      "Epoch 979/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0484 - val_loss: 0.1550\n",
      "Epoch 980/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0407 - val_loss: 0.1627\n",
      "Epoch 981/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0367 - val_loss: 0.1628\n",
      "Epoch 982/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0477 - val_loss: 0.1662\n",
      "Epoch 983/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0454 - val_loss: 0.1613\n",
      "Epoch 984/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0492 - val_loss: 0.1654\n",
      "Epoch 985/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0472 - val_loss: 0.1642\n",
      "Epoch 986/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0384 - val_loss: 0.1596\n",
      "Epoch 987/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0467 - val_loss: 0.1606\n",
      "Epoch 988/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0385 - val_loss: 0.1588\n",
      "Epoch 989/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0417 - val_loss: 0.1612\n",
      "Epoch 990/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0421 - val_loss: 0.1601\n",
      "Epoch 991/1000\n",
      "934/934 [==============================] - 31s 34ms/step - loss: 0.0514 - val_loss: 0.1583\n",
      "Epoch 992/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0409 - val_loss: 0.1824\n",
      "Epoch 993/1000\n",
      "934/934 [==============================] - 33s 35ms/step - loss: 0.0442 - val_loss: 0.1722\n",
      "Epoch 994/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0407 - val_loss: 0.1601\n",
      "Epoch 995/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0404 - val_loss: 0.1614\n",
      "Epoch 996/1000\n",
      "934/934 [==============================] - 31s 33ms/step - loss: 0.0408 - val_loss: 0.1606\n",
      "Epoch 997/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0445 - val_loss: 0.1609\n",
      "Epoch 998/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0425 - val_loss: 0.1604\n",
      "Epoch 999/1000\n",
      "934/934 [==============================] - 29s 31ms/step - loss: 0.0564 - val_loss: 0.1647\n",
      "Epoch 1000/1000\n",
      "934/934 [==============================] - 30s 32ms/step - loss: 0.0478 - val_loss: 0.1590\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim =x_train.shape[1] ))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'he_uniform',activation='linear'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss='mean_absolute_error', optimizer='Adamax')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(x_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ir=classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04219718648278235"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predict_ir,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ir=classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_test=ld_test[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ir_final=classifier.predict(ld_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.470753],\n",
       "       [11.951985],\n",
       "       [12.063473],\n",
       "       ...,\n",
       "       [11.839749],\n",
       "       [11.782703],\n",
       "       [12.356799]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ir_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_test[\"SalePrice\"]=predict_ir_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction=ld_test[['Id',\"SalePrice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>11.470753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1462</td>\n",
       "      <td>11.951985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1463</td>\n",
       "      <td>12.063473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1464</td>\n",
       "      <td>12.332918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1465</td>\n",
       "      <td>12.345942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1454</td>\n",
       "      <td>2915</td>\n",
       "      <td>11.485794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>2916</td>\n",
       "      <td>11.235067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>2917</td>\n",
       "      <td>11.839749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1457</td>\n",
       "      <td>2918</td>\n",
       "      <td>11.782703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1458</td>\n",
       "      <td>2919</td>\n",
       "      <td>12.356799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  SalePrice\n",
       "0     1461  11.470753\n",
       "1     1462  11.951985\n",
       "2     1463  12.063473\n",
       "3     1464  12.332918\n",
       "4     1465  12.345942\n",
       "...    ...        ...\n",
       "1454  2915  11.485794\n",
       "1455  2916  11.235067\n",
       "1456  2917  11.839749\n",
       "1457  2918  11.782703\n",
       "1458  2919  12.356799\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.247694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12.109011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12.317167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>11.849398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12.429216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>1456</td>\n",
       "      <td>12.072541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>1457</td>\n",
       "      <td>12.254863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1457</td>\n",
       "      <td>1458</td>\n",
       "      <td>12.493130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1458</td>\n",
       "      <td>1459</td>\n",
       "      <td>11.864462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1459</td>\n",
       "      <td>1460</td>\n",
       "      <td>11.901583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  SalePrice\n",
       "0        1  12.247694\n",
       "1        2  12.109011\n",
       "2        3  12.317167\n",
       "3        4  11.849398\n",
       "4        5  12.429216\n",
       "...    ...        ...\n",
       "1455  1456  12.072541\n",
       "1456  1457  12.254863\n",
       "1457  1458  12.493130\n",
       "1458  1459  11.864462\n",
       "1459  1460  11.901583\n",
       "\n",
       "[1460 rows x 2 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld_train[['Id',\"SalePrice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_prediction).to_csv(\"advance_house_price_prediction.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
